{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Welcome to SimplestLearn Wiki!","text":"<p>Welcome to your journey into the world of Artificial Intelligence, Machine Learning, and Deep Learning!</p> <p>This wiki is designed to teach you the fundamentals of AI/ML in the simplest, most intuitive way possible. No complex math. No boring jargon. Just clear explanations and real-world examples.</p>"},{"location":"#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<p>Our curriculum is organized into foundational lessons that build on each other. We recommend going through them in order.</p> Lesson Topic Description 1 Introduction to AI/ML/DL Understand the big picture and the relationship between AI, Machine Learning, and Deep Learning. 2 The Three Flavors of Machine Learning Discover the three main types of ML: Supervised, Unsupervised, and Reinforcement Learning. 3 Classification vs. Regression Dive into the two main problems that Supervised Learning solves: sorting things or predicting numbers. 4 Getting Your Data Ready Learn the crucial step of data preparation, including encoding and handling missing values. 5 Your First AI Model Time to code! Build your first Decision Tree model using the powerful <code>scikit-learn</code> library. 6 Decision Tree from scratch For the curious, we'll build a Decision Tree from scratch to see how the magic really works. 7 Is Our AI Smart? Learn how to evaluate your model's performance like a real scientist using a \"test set.\" 8 You Are Who Your Friends Are Explore a new, intuitive algorithm called K-Nearest Neighbors (KNN). 9 KNN from scratch A true challenge! We'll build the K-Nearest Neighbors algorithm from scratch. 10 The Art of Measurement Dive deeper into KNN and explore different ways to measure \"distance\" between data points. 11 Introduction to Neural Networks A gentle introduction to the \"brains\" of AI, from Perceptrons to Multi-Layer Perceptrons. 12 Neural Networks from scratch Pull back the curtain and build a working neural network from scratch using only NumPy."},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Start with Lesson 1 to understand the fundamentals, then progress through the lessons in order. Each one builds on the previous, taking you from a beginner to a true AI builder!</p> <p>Happy learning! Remember: everyone was a beginner once. You've got this! \ud83d\udcaa</p>"},{"location":"lessons/lesson1/","title":"Lesson 1: Your Secret Guide to Artificial Intelligence! \ud83e\udd16","text":"<p>Hey there, future tech genius! </p> <p>Ever wondered how YouTube seems to know exactly what video you want to watch next? Or how video games have enemies that seem to think for themselves? It's not magic, it's something even cooler: Artificial Intelligence, or AI for short.</p> <p>Welcome to your very first lesson where we'll uncover the secrets of AI, Machine Learning (ML), and Deep Learning (DL). We're going to skip the super-boring math and get straight to the fun, powerful ideas that are changing the world.</p>"},{"location":"lessons/lesson1/#part-1-the-big-picture-what-is-this-ai-magic","title":"Part 1: The Big Picture - What is this AI Magic?","text":"<p>Imagine you have a set of Russian nesting dolls. You know, those wooden dolls that have smaller dolls inside them? AI, ML, and DL are kind of like that!</p> <ul> <li> <p>Artificial Intelligence (AI) is the biggest, outermost doll. It's the whole grand idea of making machines smart. This could be anything from a simple calculator to a robot that can have a conversation with you. If a machine is doing something that would normally require a human brain\u2014like learning, reasoning, or problem-solving\u2014that's AI!</p> </li> <li> <p>Machine Learning (ML) is the next doll inside. This is where things get really interesting. Instead of a programmer writing a giant list of rules for every single situation, we let the machine learn on its own! We feed it a ton of information, which we call data, and it starts to figure out the patterns. </p> </li> <li> <p>Deep Learning (DL) is the smallest, most powerful doll. This is a super-special kind of machine learning that's inspired by the human brain. It's amazing at finding very tricky patterns in huge amounts of data. It's the secret sauce behind things like self-driving cars recognizing pedestrians, or voice assistants like Siri and Alexa understanding what you say.</p> </li> </ul> <p>\ud83e\udde0 Learning Like a Human!</p> <p>The way a machine \"learns\" from data is a lot like how you learned to ride a bike. No one gave you a perfect set of rules. You just tried, maybe you fell, and you adjusted. Your brain learned from the \"data\" of each attempt. Machine Learning is the same idea: learning from experience!</p> <p>Here's a little diagram to help you remember:</p> <p>Real-World Examples:</p> <ul> <li>AI: A character in a video game that cleverly finds its way around obstacles to reach you.</li> <li>ML: Your email inbox automatically sorting emails into \"spam\" and \"not spam\" based on thousands of examples it has seen before.</li> <li>DL: A website that can look at a picture of a dog and tell you what breed it is, even if it's never seen that specific dog before!</li> </ul>"},{"location":"lessons/lesson1/#part-2-a-journey-through-time-the-history-of-ai","title":"Part 2: A Journey Through Time - The History of AI","text":"<p>Artificial Intelligence isn't a recent invention; it's a field with a rich and fascinating history, marked by periods of great optimism and challenging setbacks. Understanding this journey helps us appreciate the technology we have today.</p> <p>Historical Insight: The Mechanical Turk</p> <p>In the 1770s, an automaton called \"The Mechanical Turk\" toured Europe, defeating skilled chess players. It was later revealed to be an elaborate illusion controlled by a hidden human master. While not true AI, it ignited public imagination about the possibility of intelligent machines.</p> <pre><code>graph LR\n    Turing[\"Turing Test (1950)\"] --&gt; Dartmouth[\"Dartmouth Workshop (1956)\"] --&gt; Lighthill[\"Lighthill Report (1973)\"] --&gt; DeepBlue[\"Deep Blue vs Kasparov (1997)\"] --&gt; ImageNet[\"ImageNet Challenge (2012)\"]</code></pre> <p>The history of modern AI is often divided into several key periods:</p> <ul> <li> <p>1950s: The Dawn of AI. The field was formally established. Alan Turing's seminal paper proposed the \"Turing Test\" to measure machine intelligence, and the 1956 Dartmouth Summer Research Project on Artificial Intelligence coined the term \"Artificial Intelligence\" and set the research agenda for decades.</p> </li> <li> <p>1960s-1970s: The \"AI Winter\". Initial enthusiasm led to bold predictions that were difficult to achieve with the limited computing power of the era. A lack of progress on complex problems led to reduced government funding and a period of slower research, often called the \"AI Winter.\"</p> </li> <li> <p>1980s: The Rise of Expert Systems. AI saw a resurgence with the commercial success of \"expert systems.\" These programs captured the knowledge of human experts in a specific domain (like medical diagnosis or geology) to provide automated advice and solutions.</p> </li> <li> <p>1990s-2000s: The Growth of Machine Learning. With the advent of more powerful computers and larger datasets, Machine Learning began to flourish. A landmark event was in 1997, when IBM's Deep Blue chess computer defeated world champion Garry Kasparov, demonstrating the power of computational brute force and advanced search algorithms.</p> </li> <li> <p>2010s-Present: The Deep Learning Revolution. The current era is dominated by Deep Learning. Fueled by massive datasets (like ImageNet), powerful GPUs (Graphics Processing Units), and refined algorithms, AI has achieved breakthroughs in image recognition, natural language processing, and autonomous systems, making it a part of our daily lives.</p> </li> </ul> <p>Funny Story Time: In 1952, a computer scientist named Arthur Samuel created a checkers program that could learn from its own mistakes. After playing thousands of games against itself, it eventually became better than its creator! Arthur Samuel said it was \"a very humbling experience.\" It was one of the first examples of a machine truly learning.</p>"},{"location":"lessons/lesson1/#part-3-your-first-adventure-in-machine-learning-the-case-of-the-pizza-loving-student","title":"Part 3: Your First Adventure in Machine Learning - The Case of the Pizza-Loving Student","text":"<p>Okay, enough history. Let's get our hands dirty with a real machine learning problem!</p> <p>The Mission: We need to build a model that can predict if a new student will like pizza. \ud83c\udf55</p> <p>The Clues (Our Data): We have some information about other students. This is our training data. In machine learning, the clues we use to make a prediction are called features, and the thing we're trying to predict is called the label.</p> Student Likes Video Games? (Feature 1) Likes Superheroes? (Feature 2) Likes Pizza? (Label) Alex Yes Yes Yes Ben No Yes Yes Chloe Yes No Yes David No No No Emily Yes Yes Yes <p>The Detective Work (Building a Model): Now, we need to find a pattern. Looking at the table, what do you notice?</p> <p>It seems like if a student likes video games OR superheroes, they probably like pizza. That's our model! It's a simple rule that our machine can use to make predictions.</p> <p>The Test (Is Our Model Any Good?): A new student, Frank, arrives. He loves video games but isn't a big fan of superheroes.</p> <ul> <li>Our Model: If a student likes video games OR superheroes, they will like pizza.</li> <li>Frank's Features: He likes video games.</li> <li>Prediction: Frank will like pizza!</li> </ul> <p>But wait! Another new student, Grace, arrives. She doesn't like video games or superheroes.</p> <ul> <li>Our Model: If a student likes video games OR superheroes, they will like pizza.</li> <li>Grace's Features: She doesn't like video games or superheroes.</li> <li>Prediction: Grace will not like pizza.</li> </ul> <p>Let's say we ask Grace, and she says she loves pizza! Our model was wrong! And that's one of the most important lessons in machine learning: models are not perfect. They are just our best guess based on the data we have. A big part of machine learning is testing our models and finding ways to make them better.</p>"},{"location":"lessons/lesson1/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<p>Now it's your turn to be the AI expert. Think about these questions:</p> <ol> <li>Can you think of three things you use every day that might use AI? (Hint: think about your phone, your TV, and the internet!)</li> <li>If you could teach a robot to do any chore in your house, what would it be? What kind of \"data\" would you need to give it so it could learn?</li> <li>Our pizza model wasn't perfect. What other \"features\" (clues) could we collect about students to make our pizza predictions better?</li> </ol> <p>What's Next?</p> <p>You've taken a huge step into the amazing world of AI! You've learned the difference between AI, ML, and DL, traveled through history, and even built and tested your very own machine learning model.</p> <p>Next time, we'll explore some of the different types of machine learning and start to see how a little bit of code can bring these ideas to life.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson10/","title":"Lesson 10: The Art of Measurement - Choosing the Right Friendship Bracelet \ud83d\udccf","text":"<p>Welcome back, detective!</p> <p>In our last lesson, we built an amazing KNN model from scratch. The heart of that model was the <code>euclidean_distance</code> function. But is that always the best way to measure \"friendship\"?</p> <p>Today, we're going to explore the \"Art of Measurement.\" We'll learn why choosing the right way to measure distance is critical, and we'll discover new types of \"friendship bracelets\" for different kinds of data.</p>"},{"location":"lessons/lesson10/#part-1-why-distance-can-be-tricky-feature-scaling","title":"Part 1: Why Distance Can Be Tricky (Feature Scaling)","text":"<p>Imagine we have a new dataset to predict if a person is a \"Super Gamer.\" Our features are: 1.  Hours Played Per Week: (e.g., 5, 10, 20) 2.  Has a Gaming PC: (1 for Yes, 0 for No)</p> <p>Let's compare two people: *   Alice: Plays 10 hours, has a gaming PC. <code>[10, 1]</code> *   Bob: Plays 12 hours, does not have a gaming PC. <code>[12, 0]</code></p> <p>If we use our Euclidean distance formula, the difference in hours played (<code>(12-10)^2 = 4</code>) has a much bigger impact on the distance than the difference in having a PC (<code>(0-1)^2 = 1</code>). The algorithm will think that the \"Hours Played\" feature is more important, just because it has bigger numbers!</p> <p>The Solution: Normalization (The Great Equalizer) To fix this, we use a technique called feature scaling or normalization. The goal is to get all our features onto the same scale (usually from 0 to 1).</p> <p>For example, if the maximum hours played is 20, we can divide every value in that column by 20. *   Alice's scaled data: <code>[10/20, 1]</code> = <code>[0.5, 1]</code> *   Bob's scaled data: <code>[12/20, 0]</code> = <code>[0.6, 0]</code></p> <p>Now, both features have a similar scale, and our distance calculation will be fair. This is a critical, professional step in almost all machine learning projects!</p>"},{"location":"lessons/lesson10/#part-2-more-ways-to-measure-friendship","title":"Part 2: More Ways to Measure Friendship","text":"<p>Euclidean distance is great, but it's not the only tool in our toolbox. Let's learn about a few more!</p>"},{"location":"lessons/lesson10/#1-euclidean-distance-the-ruler","title":"1. Euclidean Distance (The Ruler)","text":"<p>This is the one you know! It's the straight-line distance between two points. It's the default choice for most problems where your features are numbers of a similar scale.</p> \\[ d(p_1, p_2) = \\sqrt{\\sum_{i=1}^{n} (p_{1i} - p_{2i})^2} \\]"},{"location":"lessons/lesson10/#2-manhattan-distance-the-taxi-driver","title":"2. Manhattan Distance (The Taxi Driver)","text":"<p>The Analogy: Imagine you're in a city with a perfect grid of streets. You can't walk through buildings (like Euclidean distance), you have to walk along the blocks. This is Manhattan distance!</p> <p>It's calculated by summing the absolute differences of the features. $$ d(p_1, p_2) = \\sum_{i=1}^{n} |p_{1i} - p_{2i}| $$ *   When to use it: It's useful when your features represent different, unrelated concepts, and you don't want a large difference in one feature to dominate the others.</p>"},{"location":"lessons/lesson10/#3-cosine-similarity-the-compass","title":"3. Cosine Similarity (The Compass)","text":"<p>This one is a bit different. Instead of measuring the distance between two points, it measures the angle between them. Are they pointing in the same direction?</p> <p>The Analogy: Imagine two friends are describing a movie. *   Friend A: \"It was a cool, cool movie.\" *   Friend B: \"It was a cool, fun, awesome, great movie.\"</p> <p>If we just count words, their descriptions are different. But the meaning or direction of their comments is very similar. Cosine similarity is great at capturing this! It's not about the magnitude (how many words they used), but the direction (the sentiment).</p> <ul> <li>When to use it: This is the king of text analysis! It's used in search engines and document analysis to find texts that are about the same topic, even if they don't use the exact same words.</li> </ul>"},{"location":"lessons/lesson10/#4-hamming-distance-the-typo-detector","title":"4. Hamming Distance (The Typo Detector)","text":"<p>What if our data isn't numbers at all, but categories or words?</p> <p>The Analogy: This is like playing \"spot the difference.\" You compare two things and count how many positions are different.</p> <p>Let's compare two students based on their preferences: *   Student A: <code>[\"Likes Action\", \"Likes Comedy\", \"Hates Horror\"]</code> *   Student B: <code>[\"Likes Action\", \"Hates Comedy\", \"Hates Horror\"]</code></p> <p>The Hamming distance is 1, because they only differ in one position (\"Likes Comedy\" vs. \"Hates Comedy\").</p> <ul> <li>When to use it: It's perfect for comparing any kind of categorical data, especially strings or binary code.</li> </ul>"},{"location":"lessons/lesson10/#part-3-the-detectives-guide-when-to-use-which","title":"Part 3: The Detective's Guide - When to Use Which?","text":"Distance Metric Best For... Analogy Euclidean General-purpose, when features are similar and numerical. The Ruler Manhattan Data on a grid, or when features are not directly comparable. The Taxi Driver Cosine Text data, or when the \"direction\" matters more than the \"size.\" The Compass Hamming Categorical or string data. The Typo Detector"},{"location":"lessons/lesson10/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<ol> <li>If you were comparing two customers based on their age and their yearly income (in dollars), why would feature scaling be absolutely necessary?</li> <li>You want to build a system that recommends news articles based on an article you're currently reading. Which distance metric would be the best choice? Why?</li> <li>If you were comparing two strands of DNA, which are long strings of letters (A, C, G, T), which distance metric would you use to see how similar they are?</li> </ol> <p>What's Next?</p> <p>This officially concludes our introductory series on Machine Learning! You have journeyed from the highest-level concepts of AI to the nitty-gritty details of distance metrics and feature scaling. You now have the foundational knowledge to tackle almost any new algorithm you encounter.</p> <p>The world of AI is vast and constantly changing, but the principles you've learned here\u2014data preparation, training, testing, and choosing the right tool for the job\u2014will always be the keys to success.</p> <p>Never stop learning, never stop building, and never stop being curious!</p>"},{"location":"lessons/lesson11/","title":"Lesson 11: Building a Brain - Your Introduction to Neural Networks \ud83e\udd16\ud83e\udde0","text":"<p>Welcome back, future AI architect! So far, we've taught machines to learn from data. Now, we're going to give our machine a \"brain.\" This is the world of Neural Networks, the technology that powers everything from facial recognition to the most advanced AI art generators.</p>"},{"location":"lessons/lesson11/#part-1-the-dream-team-what-is-a-neural-network","title":"Part 1: The Dream Team - What is a Neural Network?","text":"<p>Imagine you're trying to decide if a picture shows a cat or a dog. You might assemble a team of experts.</p> <ul> <li>One expert only looks for pointy ears.</li> <li>Another only looks for whiskers.</li> <li>A third looks for the shape of the snout.</li> <li>A fourth looks for the texture of the fur.</li> </ul> <p>No single expert can make the final call, but by working together and weighing each other's opinions, they can make a very accurate guess. A neural network is exactly like this, but the \"experts\" are tiny computing units called neurons.</p> <p>These neurons are organized into layers:</p> <ol> <li>Input Layer: This is where the data comes in. For our cat/dog problem, this layer would take the raw pixels of the image.</li> <li>Hidden Layers: This is the core of the network. One or more hidden layers of \"experts\" work together to find patterns. The first hidden layer might find simple shapes (lines, curves), the next might combine those to find eyes and ears, and so on. This is where the \"deep\" in Deep Learning comes from!</li> <li>Output Layer: This layer gives the final answer. After listening to all the hidden experts, it might say, \"I'm 95% sure this is a dog.\"</li> </ol> <p>Here\u2019s what that looks like:</p> <p>\ud83e\udde0 It's All in the Connections!</p> <p>The design of neural networks is inspired by the human brain. Your brain isn't powerful because of any single neuron; it's powerful because of the trillions of connections between them. A neural network learns by strengthening or weakening the connections between its neurons, just like your brain creates and strengthens pathways as you learn a new skill.</p>"},{"location":"lessons/lesson11/#part-2-the-neurons-on-switch-activation-functions","title":"Part 2: The Neuron's \"On\" Switch - Activation Functions","text":"<p>So, what does a single neuron actually do? It performs two simple steps:</p> <ol> <li>Weighted Sum: It takes all the information from the previous layer, multiplies each piece by a weight (its importance), and sums it all up, adding a bias term. This is the neuron's raw input, often called <code>z</code>.     \\(z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b\\)</li> <li>Activation: It passes this sum <code>z</code> through an activation function to decide what signal to pass to the next layer.</li> </ol> <p>Think of the activation function as a light's dimmer switch. It decides how \"bright\" the neuron's output signal should be based on the raw input. Let's look at the most popular ones.</p>"},{"location":"lessons/lesson11/#sigmoid","title":"Sigmoid","text":"<p>The Sigmoid function takes any real number and \"squashes\" it into a range between 0 and 1. This is perfect for the output layer when you need to predict a probability (e.g., the probability of an email being spam).</p> <p>Formula: $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$</p> <p>Visualization (Sigmoid):</p> <p>As the input <code>z</code> gets very large, the output gets close to 1. As it gets very small (very negative), the output gets close to 0.</p>"},{"location":"lessons/lesson11/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<p>ReLU is the most popular activation function for hidden layers. It's incredibly simple and efficient. If the input is positive, it passes it through. If it's negative, it outputs zero.</p> <p>Formula: $$ \\text{ReLU}(z) = \\max(0, z) $$</p> <p>Visualization (ReLU):</p> <p>This \"on/off\" behavior helps the network learn faster and is less computationally expensive than Sigmoid.</p>"},{"location":"lessons/lesson11/#softmax","title":"Softmax","text":"<p>Softmax is a special one, used exclusively in the output layer for multi-class classification problems (e.g., classifying an image as a dog, cat, bird, or fish). It takes the raw outputs for all classes and converts them into a probability distribution, where all the probabilities add up to 1.</p> <p>Formula: For a set of raw outputs \\(z_1, z_2, ..., z_k\\), the Softmax for output \\(i\\) is: $$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} $$</p> <p>Visualization (Softmax example): - Example logits: z = [-1, 0.5, 2.0] - Softmax outputs \u2248 [0.04, 0.18, 0.78]</p> <p>This ensures you can see how softmax pushes logits into a probability distribution where the largest logit dominates the final output.</p>"},{"location":"lessons/lesson11/#part-3-learning-from-mistakes-how-a-network-trains","title":"Part 3: Learning from Mistakes - How a Network Trains","text":"<p>How does the network learn the right weights for its connections? It trains, just like a person.</p> <ol> <li>Forward Pass: It takes a piece of data (e.g., a picture of a dog) and makes a guess. This is called the forward pass.</li> <li>Calculate Error (Loss): It compares its guess to the correct answer. The difference is the error or loss.</li> <li>Backward Pass (Backpropagation): This is the magic! The network works backward from the error and figures out which connections were most responsible for the mistake. It then adjusts the weights on those connections to do better next time. This process is called backpropagation.</li> </ol> <p>It repeats this process thousands or millions of times, getting a little smarter with each example.</p>"},{"location":"lessons/lesson11/#part-4-lets-build-a-brain","title":"Part 4: Let's Build a Brain!","text":"<p>Enough theory! Let's build a simple neural network with <code>scikit-learn</code> to classify the famous Iris dataset.</p> <pre><code># A simple Neural Network using scikit-learn's MLPClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Load and prepare the data\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n\n# It's very important to scale your data for neural networks!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 2. Create the model\n# We'll create a network with one hidden layer containing 10 \"expert\" neurons.\nmodel = MLPClassifier(\n    hidden_layer_sizes=(10,), \n    activation='relu',          # Use the popular ReLU activation\n    max_iter=1000,              # Train for 1000 rounds\n    random_state=42\n)\n\n# 3. Train the model\nprint(\"Training the model...\")\nmodel.fit(X_train_scaled, y_train)\nprint(\"Training complete!\")\n\n# 4. Make predictions and check accuracy\ny_pred = model.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Our neural network's accuracy is: {accuracy:.2f}\")\n</code></pre>"},{"location":"lessons/lesson11/#part-5-your-turn-to-be-the-architect","title":"Part 5: Your Turn to Be the Architect!","text":"<p>Now it's your turn to experiment.</p> <ol> <li>Change the Architecture: What happens to the accuracy if you change the <code>hidden_layer_sizes</code>? Try <code>(20,)</code> or <code>(5, 5)</code> (two hidden layers with 5 neurons each).</li> <li>Try a Different \"Switch\": Change the <code>activation</code> from <code>'relu'</code> to <code>'tanh'</code>. Does it make a difference?</li> <li>The Importance of Scaling: Try running the code without the <code>StandardScaler</code>. What happens to the performance? (This is a very important lesson!)</li> </ol> <p>What's Next?</p> <p>You've just built and trained your first neural network using a high-level library! This is a huge step. But what's happening under the hood?</p> <p>Next time, we'll pull back the curtain and build our very own neural network from scratch using only Python and NumPy. This will give you a deep, fundamental understanding of how a machine truly learns.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson12/","title":"Lesson 12: The Man Behind the Curtain - Building a Neural Network from Scratch \ud83d\udee0\ufe0f","text":"<p>In our last lesson, we used a high-level library, <code>scikit-learn</code>, to create a neural network in just a few lines of code. It felt like magic! But true understanding\u2014the kind that separates a good developer from a great one\u2014comes from looking behind the curtain.</p> <p>Today, we're going to do just that. We will build our own neural network from scratch using only Python and NumPy. This process will demystify the \"magic\" and give you a deep, intuitive understanding of how a machine truly learns.</p> <p>We'll embark on a two-part journey: 1.  The Perceptron: We'll start by building the simplest possible neuron, the ancestor of all modern AI. 2.  The Multi-Layer Perceptron (MLP): We'll then evolve our simple neuron into a powerful, multi-layered network capable of solving complex problems.</p> <p>Let's begin.</p>"},{"location":"lessons/lesson12/#part-1-the-original-neuron-the-perceptron","title":"Part 1: The Original Neuron - The Perceptron","text":"<p>Invented in 1958 by Frank Rosenblatt, the Perceptron is the simplest form of a neural network: a single neuron that can make decisions.</p>"},{"location":"lessons/lesson12/#how-it-works","title":"How It Works","text":"<p>The Perceptron operates on a simple principle: it takes a set of inputs, weighs their importance, and if the combined evidence is strong enough, it \"fires.\"</p> <ol> <li>Weighted Sum: It calculates a weighted sum of its inputs (\\(x_i\\)) and adds a bias (\\(b\\)). The weights (\\(w_i\\)) represent the importance of each input, and the bias acts as a general threshold for firing.     $$ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b $$</li> <li>Activation: It passes this sum through a step function. If the sum is greater than zero, it outputs 1 (fires); otherwise, it outputs 0 (does not fire).</li> </ol>"},{"location":"lessons/lesson12/#the-code-building-a-perceptron","title":"The Code: Building a Perceptron","text":"<p>Here is the implementation in Python. The <code>fit</code> method contains the core learning algorithm.</p> <pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"A single neuron that can learn linear patterns.\"\"\"\n    def __init__(self, learning_rate=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.activation_func = self._step_function\n        self.weights = None\n        self.bias = None\n\n    def _step_function(self, x):\n        return np.where(x &gt;= 0, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\"Train the perceptron.\"\"\"\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        y_ = np.array([1 if i &gt; 0 else 0 for i in y])\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self.activation_func(linear_output)\n\n                # Perceptron update rule\n                update = self.lr * (y_[idx] - y_predicted)\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self.activation_func(linear_output)\n</code></pre>"},{"location":"lessons/lesson12/#breakdown-the-fit-method","title":"Breakdown: The <code>fit</code> Method","text":"<p>The <code>fit</code> method is where the Perceptron learns. For each training example, it performs a simple, elegant update:</p> <ol> <li>Make a Prediction: It computes the <code>linear_output</code> and passes it through the <code>_step_function</code> to get a prediction (0 or 1).     <pre><code>linear_output = np.dot(x_i, self.weights) + self.bias\ny_predicted = self.activation_func(linear_output)\n</code></pre></li> <li>Calculate the Error: It finds the difference between the true label (<code>y_[idx]</code>) and its prediction.</li> <li>Calculate the Update: It multiplies the error by the <code>learning_rate</code>. If the prediction was correct, the error is 0, and no update occurs!     <pre><code>update = self.lr * (y_[idx] - y_predicted)\n</code></pre></li> <li>Adjust Weights and Bias: It nudges the weights and bias in the direction that would reduce the error. If it guessed 0 but the answer was 1, it increases the weights and bias to make the output larger next time, and vice-versa.     <pre><code>self.weights += update * x_i\nself.bias += update\n</code></pre></li> </ol> <p>The Perceptron is powerful, but it has a famous weakness that stumped AI researchers for years.</p>"},{"location":"lessons/lesson12/#part-2-the-perceptrons-kryptonite-the-xor-problem","title":"Part 2: The Perceptron's Kryptonite - The XOR Problem","text":"<p>A single Perceptron can only learn linearly separable patterns. This means it can only succeed if it's possible to draw a single straight line to separate the different classes of data.</p> <p>This limitation is perfectly illustrated by the classic XOR problem. XOR (exclusive OR) is a logical operation where the output is true only if the inputs are different.</p> Input 1 Input 2 Output 0 0 0 0 1 1 1 0 1 1 1 0 <p>If you plot these points, you'll find it's impossible to draw one straight line to separate the 1s from the 0s. This discovery led to the first \"AI Winter,\" a period where many lost faith in the potential of neural networks.</p> <p>The solution? Don't use just one neuron. Use many.</p>"},{"location":"lessons/lesson12/#part-3-the-comeback-the-multi-layer-perceptron-mlp","title":"Part 3: The Comeback - The Multi-Layer Perceptron (MLP)","text":"<p>By adding a hidden layer of neurons, the network can learn non-linear patterns. It's like giving the Perceptron multiple lines to draw with, allowing it to create complex decision boundaries. This is a Multi-Layer Perceptron (MLP).</p> <p>To build one, we need two key upgrades: 1.  A smooth activation function (like Sigmoid) that allows us to use calculus to measure error. 2.  Backpropagation, an algorithm to figure out how much each weight and bias in the network contributed to the final error.</p>"},{"location":"lessons/lesson12/#the-code-building-an-mlp","title":"The Code: Building an MLP","text":"<p>This code looks more complex, but the principles are the same. We'll break it down completely.</p> <pre><code>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\nclass MLP:\n    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n        self.W1 = np.random.randn(input_size, hidden_size)\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size)\n        self.b2 = np.zeros((1, output_size))\n        self.lr = learning_rate\n\n    def forward(self, X):\n        self.hidden_input = np.dot(X, self.W1) + self.b1\n        self.hidden_output = sigmoid(self.hidden_input)\n        self.final_input = np.dot(self.hidden_output, self.W2) + self.b2\n        self.final_output = sigmoid(self.final_input)\n        return self.final_output\n\n    def backward(self, X, y, output):\n        output_error = y - output\n        output_delta = output_error * sigmoid_derivative(output)\n\n        hidden_error = output_delta.dot(self.W2.T)\n        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n\n        self.W2 += self.hidden_output.T.dot(output_delta) * self.lr\n        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.lr\n        self.W1 += X.T.dot(hidden_delta) * self.lr\n        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.lr\n\n    def train(self, X, y, epochs=10000):\n        for i in range(epochs):\n            output = self.forward(X)\n            self.backward(X, y, output)\n            if (i % 1000) == 0:\n                loss = np.mean(np.square(y - output))\n                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n</code></pre>"},{"location":"lessons/lesson12/#breakdown-the-forward-method","title":"Breakdown: The <code>forward</code> Method","text":"<p>This method makes a prediction by passing data through the network from start to finish.</p> <ol> <li>Hidden Layer Calculation: It computes the weighted sum for the hidden layer (<code>hidden_input</code>) and applies the sigmoid activation function to get the <code>hidden_output</code>.     <pre><code>self.hidden_input = np.dot(X, self.W1) + self.b1\nself.hidden_output = sigmoid(self.hidden_input)\n</code></pre></li> <li>Output Layer Calculation: It takes the <code>hidden_output</code> as its input, computes the final weighted sum (<code>final_input</code>), and applies sigmoid one last time to get the network's final prediction.     <pre><code>self.final_input = np.dot(self.hidden_output, self.W2) + self.b2\nself.final_output = sigmoid(self.final_input)\n</code></pre></li> </ol>"},{"location":"lessons/lesson12/#breakdown-the-backward-method-backpropagation","title":"Breakdown: The <code>backward</code> Method (Backpropagation)","text":"<p>This is the \"learning\" part. It's a brilliant application of the chain rule from calculus to figure out which connections to blame for an error.</p> <ol> <li>Error at Output Layer: It calculates the error of the final prediction (<code>output_error</code>). It then multiplies this by the derivative of the sigmoid function. This <code>output_delta</code> tells us the direction and magnitude of the error for the final layer's input.     <pre><code>output_error = y - output\noutput_delta = output_error * sigmoid_derivative(output)\n</code></pre></li> <li>Propagate Error to Hidden Layer: It \"propagates\" this error backward. It uses the <code>output_delta</code> and the weights of the second layer (<code>self.W2</code>) to calculate how much the hidden layer contributed to the mistake (<code>hidden_error</code>). This is then used to calculate <code>hidden_delta</code>.     <pre><code>hidden_error = output_delta.dot(self.W2.T)\nhidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n</code></pre></li> <li>Update Weights and Biases: Now knowing how much each layer was \"at fault,\" it nudges the weights and biases in the direction that will reduce the error on the next attempt.     <pre><code># Update weights and biases for the hidden-to-output connection\nself.W2 += self.hidden_output.T.dot(output_delta) * self.lr\n# ...and so on for the other weights and biases.\n</code></pre></li> </ol> <p>By repeating this forward and backward pass thousands of times, the network slowly minimizes its error and masters the task.</p>"},{"location":"lessons/lesson12/#part-4-your-mission","title":"Part 4: Your Mission","text":"<p>You've just built a working neural network from scratch! Now it's time to experiment.</p> <ol> <li>Tweak the Hyperparameters: In the MLP, what happens if you change the <code>learning_rate</code>? What about the number of neurons in the <code>hidden_size</code>?</li> <li>Break the Perceptron: Try to train the Perceptron on the XOR dataset. What happens? Why?</li> <li>Visualize: If you're feeling adventurous, try to plot the decision boundary of the trained MLP on the XOR data. You'll see how it creates a non-linear separation!</li> </ol> <p>What's Next?</p> <p>You now have a deep, fundamental understanding of how a neural network learns. You've seen the \"magic\" of backpropagation in action. With this foundation, you're more than ready to tackle the complex, large-scale architectures like CNNs that we'll explore next.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson2/","title":"Lesson 2: The Three Flavors of Machine Learning! \ud83c\udf66","text":"<p>Hey again, future tech genius!</p> <p>Last time, we learned what AI is and how it's all related. But it turns out, \"learning\" can mean a few different things for a computer.</p> <p>Think of it like ice cream. You might have chocolate, vanilla, and strawberry. They're all ice cream, but they're different flavors with different recipes. Machine learning is the same! There are three main \"flavors,\" or types, of machine learning. Let's grab a spoon and dig in!</p>"},{"location":"lessons/lesson2/#part-1-supervised-learning-the-copycat","title":"Part 1: Supervised Learning (The Copycat)","text":"<p>This is the most common type of machine learning. Supervised Learning is like studying for a test with flashcards. On one side of the card, you have a question (the data), and on the other side, you have the answer (the label).</p> <p>You show the computer thousands and thousands of these \"flashcards.\" After a while, it learns the relationship between the questions and the answers so well that it can start predicting the answer for new questions it has never seen before.</p> <p>The Big Idea: You supervise the computer by giving it all the correct answers to learn from.</p> <p>Real-World Example: Is it a Cat or a Dog? \ud83d\udc31\ud83d\udc36 Imagine you want to teach a computer to recognize photos of cats and dogs. 1.  You get the data: You collect thousands of pictures of cats and thousands of pictures of dogs. 2.  You label it: You go through every picture and label it \"cat\" or \"dog.\" This is the answer key! 3.  You train the model: You feed all these labeled pictures to your machine learning model. It studies the pictures, looking for patterns. It might learn that \"cats have pointy ears\" and \"dogs have floppy ears,\" but it will find thousands of patterns that are way more complex. 4.  You test it: Now, you show it a brand new picture of a cat it has never seen before. Because it has learned the patterns, it can confidently predict, \"That's a cat!\"</p> <p>Our pizza-predictor from Lesson 1 was a simple example of supervised learning!</p> <p>\ud83e\udd2f Fun Fact: The Spam Slayer!</p> <p>One of the first major successes of supervised learning was in the 1990s for filtering spam emails. By training a model on thousands of emails labeled \"spam\" or \"not spam,\" they were able to create filters that could automatically clean up our inboxes. It's a true hero of the internet!</p>"},{"location":"lessons/lesson2/#part-2-unsupervised-learning-the-detective","title":"Part 2: Unsupervised Learning (The Detective \ud83d\udd75\ufe0f)","text":"<p>What if you don't have an answer key? What if you just have a giant pile of data and you want the computer to find any interesting patterns or groups on its own? That's Unsupervised Learning!</p> <p>This is like being given a giant box of mixed-up LEGOs and being told to \"sort them.\" You might sort them by color, by size, or by shape. You're finding the structure all by yourself, without any instructions.</p> <p>The Big Idea: The computer is unsupervised. It has to find the hidden patterns and groupings in the data on its own.</p> <p>Real-World Example: Discovering New Music \ud83c\udfb6 Have you ever wondered how Spotify or Apple Music are so good at recommending new songs you might like? They use unsupervised learning! 1.  They get the data: They look at the listening habits of millions of people. What songs do people listen to together? What artists do the same people like? 2.  The model finds clusters: The unsupervised learning model dives into this data and starts to group songs together. It might create a cluster of high-energy dance songs, a cluster of calm study music, and a cluster of classic rock songs. It doesn't know they are \"dance\" or \"rock,\" it just knows that people who listen to one song in the cluster tend to listen to the others. 3.  You get a recommendation: When you listen to a song from one of those clusters, the service can recommend other songs from that same cluster, thinking you'll probably like them too!</p> <p>\ud83e\udde0 Finding Your Friend Group</p> <p>Think about the first day at a new school. You don't have a label for who should be your friend. You just observe people (the data!) and naturally start to see groups, or \"clusters,\" of people who share similar interests, like kids who like skateboarding, or kids who like reading. That's you doing unsupervised learning!</p>"},{"location":"lessons/lesson2/#part-3-reinforcement-learning-the-pet-trainer","title":"Part 3: Reinforcement Learning (The Pet Trainer \ud83d\udc3e)","text":"<p>This is the coolest and most futuristic type of learning. Reinforcement Learning is all about learning by trial and error, just like training a pet.</p> <p>You put an AI \"agent\" in an environment (like a video game) and give it a goal. When it does something that gets it closer to the goal, you give it a reward (like a treat). When it does something wrong, you might give it a small penalty. Over time, the agent learns to take actions that get it the most rewards.</p> <p>The Big Idea: The agent learns the best actions to take through a system of rewards and punishments, reinforcing good behavior.</p> <p>Real-World Example: A Video Game Pro \ud83c\udfae</p> <p>Imagine an AI learning to play a car racing game.</p> <ol> <li> <p>The Goal: Finish the race as fast as possible.</p> </li> <li> <p>The Environment: The racetrack.</p> </li> <li> <p>The Rewards: The AI gets points for staying on the track, for going fast, and gets a huge bonus for finishing first. It loses points for crashing.</p> </li> <li> <p>The Learning: At first, the AI is terrible! It crashes constantly. But it learns that crashing is bad (negative reward) and staying on the track is good (positive reward). After playing millions of games, it learns the perfect line to take on every turn and becomes an unbeatable racing champion. This is how AI has learned to master complex games like Chess and Go!</p> </li> </ol> <p>\ud83e\udde0 Learning to Walk</p> <p>When a baby learns to walk, they are using reinforcement learning! They take a step (an action) and either stay up (a reward!) or fall down (a penalty). Each attempt teaches their brain what to do differently next time. Reinforcement learning is the most \"natural\" way that humans and animals learn new skills.</p>"},{"location":"lessons/lesson2/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<ol> <li>Which of the three \"flavors\" of machine learning do you think is the most interesting? Why?</li> <li>If you were to build a spam filter for your email, which type of learning would you use? What would be the \"data\" and the \"labels\"?</li> <li>Can you think of a game where you learned to get better through trial and error (reinforcement learning)?</li> </ol> <p>What's Next?</p> <p>Incredible! You're now an expert on the three main ways machines learn. You've seen how these different \"flavors\" are used all around us, from our photo apps to our music playlists.</p> <p>Next time, we'll start to think about how we can actually write some simple code to make these ideas come to life. Get ready to become a real AI builder!</p> <p>Stay curious!</p>"},{"location":"lessons/lesson3/","title":"Lesson 3: The Two Big Problems ML Can Solve! \ud83c\udfaf","text":"<p>Hello again, super-learner!</p> <p>So far, we've learned what AI is and explored the three \"flavors\" of machine learning. We paid special attention to Supervised Learning, where we teach a computer using an answer key.</p> <p>Today, we're going to zoom in on Supervised Learning and discover the two main types of problems it's really good at solving: Classification and Regression. It sounds fancy, but it's as simple as sorting laundry and guessing numbers!</p>"},{"location":"lessons/lesson3/#part-1-classification-the-sorter","title":"Part 1: Classification (The Sorter \ud83e\uddfa)","text":"<p>Classification is all about teaching a machine to sort things into groups or categories. The answer is always one of a few specific choices.</p> <p>The Analogy: Sorting Laundry Imagine you have a giant pile of laundry. Your job is to sort it into the right baskets: one for whites, one for colors, and one for darks. That's classification! You're assigning each piece of clothing to a specific category.</p> <p>The Big Idea: The model predicts a category or a class.</p> <p>Real-World Examples:</p> <ul> <li>Spam or Not Spam? Your email uses classification to read an incoming email and decide: should it go in the \"spam\" basket or the \"inbox\" basket?</li> <li>Cat vs. Dog: Our example from last lesson was a classic classification problem. The model looks at a picture and sorts it into the \"cat\" category or the \"dog\" category.</li> <li>Pizza Lover?: Our very first model was also a classification model! It predicted whether a student belonged to the \"likes pizza\" group or the \"doesn't like pizza\" group.</li> </ul> <p>\ud83e\udd2f Fun Fact: Classifying the Stars!</p> <p>Astronomers use classification models to automatically sort images of distant galaxies. Based on features like the galaxy's shape and color, the model can classify it as a \"spiral,\" \"elliptical,\" or \"irregular\" galaxy, helping scientists understand the universe faster than ever before!</p>"},{"location":"lessons/lesson3/#part-2-regression-the-guesser","title":"Part 2: Regression (The Guesser \ud83d\udd22)","text":"<p>Regression is all about teaching a machine to predict a specific number or value. The answer can be any number within a range.</p> <p>The Analogy: Guessing Jellybeans Imagine a big jar full of jellybeans. If you try to guess exactly how many are inside, you're making a regression guess. Your answer isn't a category like \"a lot\" or \"a little,\" but a specific number, like \"852 jellybeans.\"</p> <p>The Big Idea: The model predicts a numerical value.</p> <p>Real-World Examples:</p> <ul> <li>Weather Forecast: When your weather app says it will be 75\u00b0F tomorrow, that's a regression model at work. It's predicting a specific number on the temperature scale.</li> <li>Pizza Delivery Time: When you order a pizza and the app says it will arrive in 25 minutes, a regression model has predicted that number based on things like traffic, how busy the store is, and how far away you are.</li> <li>House Prices: Real estate websites use regression to predict how much a house is worth. They look at features like the number of bedrooms, the square footage, and the neighborhood to predict a specific price.</li> </ul>"},{"location":"lessons/lesson3/#a-quick-visual-guide","title":"A Quick Visual Guide","text":"<p>Here\u2019s a little diagram to help you remember the difference:</p> <pre><code>graph TD\n    subgraph Classification [Sorting into Groups]\n        direction LR\n        A[Input Data &lt;br&gt; e.g., a picture of an animal] --&gt; B{Model};\n        B --&gt; C((Group A &lt;br&gt; 'Cat'));\n        B --&gt; D((Group B &lt;br&gt; 'Dog'));\n        B --&gt; E((Group C &lt;br&gt; 'Bird'));\n    end\n\n    subgraph Regression [Predicting a Number]\n        direction LR\n        F[Input Data &lt;br&gt; e.g., size of a house] --&gt; G{Model};\n        G --&gt; H[Predicted Value &lt;br&gt; e.g., $250,000];\n        H --&gt; I[...$240k ... $250k ... $260k...];\n    end</code></pre>"},{"location":"lessons/lesson3/#part-3-how-to-choose-your-ml-detective-kit","title":"Part 3: How to Choose - Your ML Detective Kit \ud83d\udd75\ufe0f\u200d\u2640\ufe0f","text":"<p>So, how do you know if you have a classification or a regression problem? It's easy! You just have to ask yourself one simple question.</p> <p>The One Big Question: Is the answer I want to predict a word/category, or is it a number?</p> <pre><code>graph TD\n    A{What kind of answer &lt;br&gt; do I want to predict?} --&gt; B{Is it a word or category?};\n    A --&gt; C{Is it a number?};\n    B -- Yes --&gt; D[It's Classification!];\n    C -- Yes --&gt; E[It's Regression!];</code></pre> <p>If the answer you're looking for is a label from a small group of choices (like \"pass\" or \"fail,\" \"cat\" or \"dog,\" \"spam\" or \"not spam\"), you've got a Classification problem.</p> <p>If the answer you're looking for is a number that can change (like a price, a temperature, or a score), you've got a Regression problem. It's that simple!</p> <p>\ud83e\udde0 Two Kinds of Questions</p> <p>Our brains do this all the time! When you look at a traffic light, you're solving a classification problem: is it \"Red,\" \"Yellow,\" or \"Green\"? When you try to guess how many minutes are left in class, you're solving a regression problem. We're constantly switching between these two types of thinking without even realizing it!</p>"},{"location":"lessons/lesson3/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<p>Time to put on your thinking cap! For each of these problems, is it a Classification problem or a Regression problem? Use your new detective kit to decide!</p> <ol> <li>Predicting whether a student will pass or fail a test.</li> <li>Predicting a student's exact score on that test (from 0 to 100).</li> <li>Predicting if a customer will buy a product (yes or no).</li> <li>Predicting how much money a customer will spend.</li> </ol> <p>What's Next?</p> <p>You've done it again! You now know the two biggest types of problems that supervised machine learning can solve, and you even know how to tell them apart. This is a huge step in understanding how AI is used to make predictions all around us.</p> <p>Next time, we'll start to get our hands dirty with a little bit of code. We'll see how we can represent our data in a way a computer can understand and take the first steps toward building a real machine learning model.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson4/","title":"Lesson 4: Getting Your Data Ready - The Secret Ingredient! \ud83c\udf73","text":"<p>Welcome back, data detective!</p> <p>In our last few lessons, we've learned what AI is, the different ways machines can learn, and the types of problems they can solve. We've talked a lot about \"data,\" but what is it, really? And how do we get it ready for a computer to understand?</p> <p>Today, we're going into the kitchen! We're going to learn how to prepare our data, which is the most important ingredient in any machine learning recipe.</p>"},{"location":"lessons/lesson4/#part-1-what-is-data-really-the-recipe-for-ml","title":"Part 1: What is Data, Really? (The Recipe for ML)","text":"<p>Think of a machine learning model as a recipe for a cake.</p> <ul> <li>The ingredients you put in are your data.</li> <li>The final cake you bake is your prediction.</li> </ul> <p>If you use bad ingredients (bad data), you're going to get a bad cake (a bad prediction), no matter how good your recipe is! This is a famous saying in machine learning: \"Garbage in, garbage out.\"</p> <p>Our data is just organized information. For our pizza-loving student problem, our data was a table of information about each student. The \"ingredients\" we used were whether they liked video games or superheroes. These ingredients are our features. The \"cake\" we were trying to bake was predicting whether they liked pizza. This is our label.</p>"},{"location":"lessons/lesson4/#part-2-from-words-to-numbers-teaching-a-computer-to-read","title":"Part 2: From Words to Numbers (Teaching a Computer to Read)","text":"<p>Here's a super important secret about computers: they can't read words, they can only understand numbers.</p> <p>If we show a computer a table with \"Yes\" and \"No,\" it has no idea what that means. We need to translate our words into a language the computer can speak. This is called encoding.</p> <p>Let's take our pizza dataset from Lesson 1.</p> <p>Before (Human-Readable):</p> Student Likes Video Games? Likes Superheroes? Likes Pizza? Alex Yes Yes Yes Ben No Yes Yes Chloe Yes No Yes David No No No <p>To translate this, we can make a simple rule: *   <code>Yes</code> will become <code>1</code> *   <code>No</code> will become <code>0</code></p> <p>After (Computer-Readable):</p> Student Likes Video Games? Likes Superheroes? Likes Pizza? Alex 1 1 1 Ben 0 1 1 Chloe 1 0 1 David 0 0 0 <p>Now that's something a computer can work with! We've successfully encoded our features and labels into numbers.</p> <p>\ud83e\udd2f Fun Fact: The First Encoders!</p> <p>The idea of encoding isn't new! The telegraph, invented in the 1830s, used Morse Code to turn letters of the alphabet into a series of dots and dashes (long and short signals). This allowed people to send complex messages over a simple wire. Encoding data for a computer is the modern version of the same idea!</p>"},{"location":"lessons/lesson4/#part-3-the-case-of-the-missing-clue","title":"Part 3: The Case of the Missing Clue \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>In the real world, data is often messy. What happens if you forget to ask a student a question? You end up with a missing value!</p> Student Likes Video Games? Likes Superheroes? Frank Yes ??? <p>A computer will see that <code>???</code> and have no idea what to do. It needs a complete table of numbers. We have two simple ways to solve this:</p> <ol> <li>Remove the Row: The easiest option is to just remove Frank from our data. But if we have a lot of missing values, we could end up throwing away too much good information!</li> <li>Make a Smart Guess (Imputation): A better way is to make a smart guess. Let's look at our other students. If most of them like superheroes, it's a safe bet that Frank might, too. We can fill in the blank with the most common value.</li> </ol> <p>Let's say most of our students like superheroes. We can \"impute\" the missing value:</p> <p>After (Imputed):</p> Student Likes Video Games? Likes Superheroes? (Imputed) Frank Yes Yes <p>Now our table is complete, and the computer can get back to work!</p> <p>\ud83e\udde0 Filling in the Blanks</p> <p>Your brain does imputation all the time! If someone says, \"I'm going to the store to buy some bread and ____,\" your brain instantly fills in the blank with something like \"milk\" or \"eggs.\" You're using your past experience (your data!) to make a smart guess about the missing information.</p>"},{"location":"lessons/lesson4/#part-4-the-mighty-table-features-and-labels-revisited","title":"Part 4: The Mighty Table (Features and Labels Revisited)","text":"<p>Let's look at our new, computer-friendly table. In machine learning, you'll often see this table split into two parts:</p> <ol> <li>The Features Table (The Clues): This is all the information we're using to make our prediction. It's usually called <code>X</code> in the math world.</li> </ol> Likes Video Games? Likes Superheroes? 1 1 0 1 1 0 0 0 <ol> <li>The Label Column (The Answer): This is the one thing we're trying to predict. It's usually called <code>y</code>.</li> </ol> Likes Pizza? 1 1 1 0 <p>When we build a machine learning model, we're essentially telling the computer: \"Hey, look at the patterns in the <code>X</code> table and see if you can figure out how to predict the <code>y</code> column.\"</p>"},{"location":"lessons/lesson4/#part-5-lets-discuss","title":"Part 5: Let's Discuss!","text":"<ol> <li>Why is it so important to have \"good ingredients\" (good data) when building a machine learning model?</li> <li>Imagine you have a feature for a t-shirt size (\"Small,\" \"Medium,\" \"Large\"). Would you use One-Hot Encoding, or could you use <code>1</code>, <code>2</code>, and <code>3</code>? Why?</li> <li>If you wanted to predict a student's grade on a test (a regression problem), what would your <code>y</code> (label) be? What are some <code>X</code> (features) you could use to predict it?</li> </ol> <p>What's Next?</p> <p>This is a huge step! You now understand the secret language of data that computers use. You know how to take real-world information, handle messy situations, and turn it all into numbers that a machine learning model can learn from.</p> <p>You are officially ready to build your first real machine learning model. In our next lesson, we'll use a simple but powerful tool to take our encoded pizza data and create a model that can make predictions, all with just a few lines of code!</p> <p>Get excited!</p>"},{"location":"lessons/lesson5/","title":"Lesson 5: Your First AI Model - The Super-Smart Flowchart! \ud83c\udf33","text":"<p>Welcome back, future AI architect!</p> <p>This is the moment we've been building up to. We've learned what AI is, how machines learn, and how to prepare our data. Today, we're going to combine all that knowledge and build our very first real machine learning model using code!</p> <p>Don't worry, we're going to start with a model that's super intuitive and easy to understand: the Decision Tree.</p>"},{"location":"lessons/lesson5/#part-1-what-is-a-decision-tree-a-20-questions-game","title":"Part 1: What is a Decision Tree? (A \"20 Questions\" Game)","text":"<p>A Decision Tree is basically a flowchart that the computer learns all by itself. It's like playing a game of \"20 Questions.\" You ask a series of yes/no questions to narrow down the possibilities and arrive at a final answer.</p> <p>Imagine you're trying to guess an animal. You might ask: *   \"Does it live in the water?\" (No) *   \"Does it have four legs?\" (Yes) *   \"Does it bark?\" (Yes) *   \"It must be a dog!\"</p> <p>A Decision Tree does the exact same thing with data! It learns the best questions to ask to separate the data into different groups.</p> <p>\ud83e\udde0 How You Decide What to Eat</p> <p>Your brain uses a decision tree every day! When you're hungry, you might think: \"Am I in the mood for something sweet or savory?\" If savory, \"Do I want something quick or a full meal?\" If quick, \"Maybe a sandwich.\" You're following the branches of your own mental flowchart to make a decision!</p>"},{"location":"lessons/lesson5/#part-2-building-the-tree-from-data-to-flowchart","title":"Part 2: Building the Tree (From Data to Flowchart)","text":"<p>Let's go back to our trusty pizza-lover dataset. Remember, we encoded it into numbers:</p> Likes Video Games? Likes Superheroes? Likes Pizza? 1 1 1 0 1 1 1 0 1 0 0 0 <p>A Decision Tree model would look at this data and figure out the best question to ask first. It might learn that asking \"Likes Video Games?\" is a good starting point. It then splits the data into two groups based on the answer. It continues asking questions until it has sorted all the students.</p> <p>The result is a flowchart that the computer builds automatically. It might look something like this:</p> <pre><code>graph TD\n    A{Likes Video Games?} --&gt;|\"Yes (1)\"| B{Likes Superheroes?};\n    A --&gt;|\"No (0)\"| C{Likes Superheroes?};\n    B --&gt;|\"Yes (1)\"| D[\"Predict: Likes Pizza! (1)\"];\n    B --&gt;|\"No (0)\"| E[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"Yes (1)\"| F[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"No (0)\"| G[\"Predict: Does NOT Like Pizza! (0)\"];</code></pre> <p>This is the \"brain\" of our model! It's a set of rules the computer has learned from the data.</p>"},{"location":"lessons/lesson5/#part-3-making-a-prediction-following-the-branches","title":"Part 3: Making a Prediction (Following the Branches)","text":"<p>Now, let's use our tree to predict if a new student, Frank, will like pizza.</p> <ul> <li>Frank's Data: He likes video games (<code>1</code>), but not superheroes (<code>0</code>).</li> </ul> <p>We start at the top of the tree: 1.  Likes Video Games? Frank's answer is Yes. We follow the \"Yes\" branch. 2.  Likes Superheroes? Frank's answer is No. We follow the \"No\" branch. 3.  Final Answer: We land on a prediction: Likes Pizza!</p> <p>It's that simple! We just followed the path down the tree to get our answer.</p>"},{"location":"lessons/lesson5/#part-4-the-code-part-python-scikit-learn","title":"Part 4: The \"Code\" Part (Python &amp; Scikit-learn)","text":"<p>Okay, it's time. How do we actually tell a computer to do this? We use a programming language, and the most popular one for AI is Python.</p> <p>But we don't have to build everything from scratch! We can use a special \"toolbox\" for machine learning called scikit-learn. It's a free library that has all the common ML models, including Decision Trees, ready to go.</p> <p>\ud83e\udd2f Fun Fact: Built by Volunteers!</p> <p>Scikit-learn, one of the most important and powerful AI toolboxes in the world, is built and maintained almost entirely by volunteers! It's a global community of people who are passionate about making machine learning accessible to everyone.</p> <p>Here\u2019s what the code would look like. Don't worry about understanding every single word. Just focus on the three main steps and the comments that explain them.</p> <pre><code># Step 1: Prepare the data (just like we did in Lesson 4!)\n# X is our features (the clues), y is our label (the answer)\nX = [[1, 1], [0, 1], [1, 0], [0, 0]]  # [Likes Games, Likes Heroes]\ny = [1, 1, 1, 0]                     # Likes Pizza (1=Yes, 0=No)\n\n# Step 2: Create and train the model\n# We'll import the Decision Tree model from our scikit-learn toolbox\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a new, empty Decision Tree model\nmy_model = DecisionTreeClassifier()\n\n# This is the magic \"learning\" step!\n# We tell the model to \"fit\" itself to our data.\nmy_model.fit(X, y)\n\n# Step 3: Make a prediction for a new student\n# Let's test it on Frank, who likes games (1) but not heroes (0)\nfrank = [[1, 0]]\nprediction = my_model.predict(frank)\n\n# See the result!\nif prediction == 1:\n    print(\"The model predicts Frank will like pizza!\")\nelse:\n    print(\"The model predicts Frank will NOT like pizza.\")\n\n# The output will be: The model predicts Frank will like pizza!\n</code></pre>"},{"location":"lessons/lesson5/#a-closer-look-what-do-fit-and-predict-actually-do","title":"A Closer Look: What Do <code>.fit()</code> and <code>.predict()</code> Actually Do?","text":"<p>Those two lines of code, <code>my_model.fit(X, y)</code> and <code>my_model.predict(frank)</code>, are the heart of the whole process. Let's use an analogy to understand them better.</p> <ul> <li> <p><code>.fit(X, y)</code> is for FORGING THE KEY \ud83d\udd11     This is the \"study\" or \"training\" phase. Think of it like a locksmith studying a lock (the <code>y</code> labels) and the original key (<code>X</code> features). The locksmith looks at all the bumps and grooves to understand the pattern. <code>fit</code> is the moment the computer does the same thing with our data. It looks at our features and labels and builds the Decision Tree flowchart. Before <code>.fit()</code>, <code>my_model</code> is just an empty shell. After <code>.fit()</code>, it's a fully trained model that has forged a \"key\" based on the patterns in the data.</p> </li> <li> <p><code>.predict(new_data)</code> is for UNLOCKING THE DOOR \ud83d\udeaa     This is the \"doing\" or \"testing\" phase. Now you have a new door (<code>frank</code>'s data), but you don't know what's behind it. You take the key you just forged (<code>my_model</code>) and use it on the new lock. <code>.predict()</code> is the action of turning the key. The result\u2014whether the door opens or not\u2014is your prediction!</p> </li> </ul>"},{"location":"lessons/lesson5/#part-5-lets-discuss","title":"Part 5: Let's Discuss!","text":"<ol> <li>Think about how you decide what to wear in the morning. What \"decision tree\" do you use? (e.g., Is it cold? -&gt; Yes -&gt; Wear a jacket).</li> <li>Our tree is very simple. What could go wrong if we used it to predict for all new students? (Hint: Think about Grace from Lesson 1).</li> <li>Does the code seem more or less complicated than you expected? What part is the most surprising?</li> </ol> <p>What's Next?</p> <p>CONGRATULATIONS! You have officially learned how to build and train your very first AI model. You've taken data, chosen a model, trained it, and used it to make a prediction. This is the core of what machine learning is all about.</p> <p>From here, you can explore other types of models, learn how to handle much bigger datasets, and even figure out how to measure if your model is any good. The journey is just beginning!</p> <p>You are no longer just a student of AI\u2014you are a builder. Stay curious!</p>"},{"location":"lessons/lesson6/","title":"Lesson 6: Under the Hood - Building a Decision Tree by Hand! \ud83d\udee0\ufe0f","text":"<p>Welcome back, AI builder!</p> <p>In our last lesson, we used the power of <code>scikit-learn</code> to build a Decision Tree model in just a few lines of code. It felt a bit like magic, right? You call <code>.fit()</code> and suddenly the computer has \"learned.\"</p> <p>Today, we're going to become the magicians. We're going to pull back the curtain and see what's happening inside that <code>.fit()</code> method. We'll build our very own Decision Tree from scratch using nothing but basic Python.</p>"},{"location":"lessons/lesson6/#part-1-from-flowchart-to-code","title":"Part 1: From Flowchart to Code","text":"<p>Let's look at our Decision Tree flowchart from the last lesson one more time.</p> <pre><code>graph TD\n    A{Likes Video Games?} --&gt;|\"Yes (1)\"| B{Likes Superheroes?};\n    A --&gt;|\"No (0)\"| C{Likes Superheroes?};\n    B --&gt;|\"Yes (1)\"| D[\"Predict: Likes Pizza! (1)\"];\n    B --&gt;|\"No (0)\"| E[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"Yes (1)\"| F[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"No (0)\"| G[\"Predict: Does NOT Like Pizza! (0)\"];</code></pre> <p>This flowchart is just a set of rules. We can read it like a story: \"First, ask if the student likes video games. If the answer is yes, then ask if they like superheroes...\" and so on.</p> <p>We can translate this exact story into Python using <code>if/else</code> statements.</p>"},{"location":"lessons/lesson6/#part-2-the-rules-in-python","title":"Part 2: The \"Rules\" in Python","text":"<p>Let's create our own prediction function called <code>predict_pizza_by_hand</code>. This function will take a student's data as a list (e.g., <code>[1, 0]</code>) and follow our rules.</p> <p>The first element in the list (<code>student[0]</code>) will be \"Likes Video Games?\", and the second element (<code>student[1]</code>) will be \"Likes Superheroes?\".</p> <pre><code>def predict_pizza_by_hand(student):\n    # The first question in our flowchart\n    if student[0] == 1:  # Do they like video games?\n        # If YES, we go down the left side of the tree.\n        # Now we ask the second question.\n        if student[1] == 1:  # Do they also like superheroes?\n            return \"Likes Pizza!\"\n        else:  # They don't like superheroes.\n            return \"Likes Pizza!\"\n    else:  # They do NOT like video games.\n        # If NO, we go down the right side of the tree.\n        # Now we ask the second question.\n        if student[1] == 1:  # Do they like superheroes?\n            return \"Likes Pizza!\"\n        else:  # They don't like superheroes either.\n            return \"Does NOT Like Pizza!\"\n</code></pre> <p>Look at that! The <code>if/else</code> statements are a perfect mirror of the branches in our flowchart. We've just manually coded the \"brain\" of our Decision Tree.</p>"},{"location":"lessons/lesson6/#part-3-putting-it-all-together","title":"Part 3: Putting It All Together","text":"<p>Now let's use our hand-built model to make a prediction, just like we did in the last lesson.</p> <pre><code># This is our hand-built model!\ndef predict_pizza_by_hand(student):\n    if student[0] == 1:\n        if student[1] == 1:\n            return \"Likes Pizza!\"\n        else:\n            return \"Likes Pizza!\"\n    else:\n        if student[1] == 1:\n            return \"Likes Pizza!\"\n        else:\n            return \"Does NOT Like Pizza!\"\n\n# Let's test it on Frank, who likes games (1) but not heroes (0)\nfrank = [1, 0]\nprediction = predict_pizza_by_hand(frank)\n\n# See the result!\nprint(\"Our hand-built model predicts Frank:\", prediction)\n\n# Let's test it on David, who doesn't like games (0) or heroes (0)\ndavid = [0, 0]\nprediction = predict_pizza_by_hand(david)\nprint(\"Our hand-built model predicts David:\", prediction)\n\n# The output will be:\n# Our hand-built model predicts Frank: Likes Pizza!\n# Our hand-built model predicts David: Does NOT Like Pizza!\n</code></pre> <p>It works perfectly! We have successfully created a machine learning model from scratch that can make predictions based on our specific rules.</p>"},{"location":"lessons/lesson6/#part-4-the-aha-moment-why-we-use-toolboxes","title":"Part 4: The \"Aha!\" Moment - Why We Use Toolboxes","text":"<p>This is great, but you might be thinking: \"If we can do this by hand, why do we need <code>scikit-learn</code>?\"</p> <p>That's the most important question!</p> <p>Our hand-built model is not learning. We looked at the data and we decided what the rules should be. We hard-coded the <code>if/else</code> statements ourselves. If we got new data, our model wouldn't change.</p> <p>The magic of <code>scikit-learn</code>'s <code>.fit()</code> method is that it does all of this for us! It looks at the data and automatically discovers the best <code>if/else</code> rules to use. It figures out the best questions to ask and in what order to make the most accurate predictions.</p> <p>If our data had 100 features, we would go crazy trying to write all the <code>if/else</code> statements by hand. But <code>.fit()</code> can figure it out in a fraction of a second.</p> <p>So, by building a model by hand, you've learned the secret of what a Decision Tree really is: it's just a smart set of <code>if/else</code> rules. And now you can truly appreciate why toolboxes like <code>scikit-learn</code> are so powerful\u2014they are masters at finding those rules for us.</p>"},{"location":"lessons/lesson6/#part-5-building-a-real-learning-algorithm","title":"Part 5: Building a Real Learning Algorithm","text":"<p>The Challenge: Welcome to the deep end of the pool! The rest of this lesson is a challenge. It's a peek into how a real learning algorithm works. It uses some more advanced ideas, but if you take it slow, you'll be able to see the real magic behind machine learning. Don't worry if you don't get it all on the first try\u2014the goal is to see how it's done!</p>"},{"location":"lessons/lesson6/#tool-1-recursion-the-endless-mirror","title":"Tool 1: Recursion (The Endless Mirror)","text":"<p>Imagine you're standing between two mirrors. You see a reflection of a reflection of a reflection... forever! That's recursion. In programming, it's a function that calls itself to solve a smaller piece of the same problem. We'll use this to \"grow\" our tree, with each branch being a smaller version of the tree.</p>"},{"location":"lessons/lesson6/#tool-2-data-structures-the-trees-blueprint","title":"Tool 2: Data Structures (The Tree's Blueprint)","text":"<p>How do we store a tree in code? We can use a class! A <code>class</code> is like a blueprint for creating objects. We'll create a <code>Node</code> class that acts as a blueprint for each question (or \"node\") in our tree. Each <code>Node</code> object will know what its question is, and what its \"left\" and \"right\" branches are.</p>"},{"location":"lessons/lesson6/#tool-3-gini-impurity-the-purity-score","title":"Tool 3: Gini Impurity (The \"Purity\" Score)","text":"<p>This is the secret sauce. How does the algorithm know which question is best? It uses a \"purity score\" to check how well a question splits the data. *   A perfectly pure group has a score of 0 (e.g., a group of students who all like pizza). *   A totally mixed group has a score of 0.5 (e.g., a group with 50% pizza lovers and 50% pizza haters).</p> <p>The algorithm calculates the purity score for every possible question and picks the one that creates the purest groups.</p> <p>\ud83e\udd2f Fun Fact: It's All About Information!</p> <p>The idea of \"Information Gain\" (which we get by lowering the Gini Impurity) was developed by a scientist named Claude Shannon, who is considered the \"father of the information age.\" He created a whole field of science called \"Information Theory\" to measure information, and it's the foundation for how computers, phones, and the internet work today!</p>"},{"location":"lessons/lesson6/#the-code-step-by-step-a-visual-walkthrough","title":"The Code, Step-by-Step: A Visual Walkthrough","text":"<p>Let's trace the computer's \"thinking\" as it starts to build the tree.</p> <p>Input: Our full dataset of 4 students. <pre><code>[[1, 1, 1],\n [0, 1, 1],\n [1, 0, 1],\n [0, 0, 0]]\n</code></pre> The computer's first goal is to find the single best question to ask to split this group.</p> <p>Step 1: Calculate the Starting Purity First, the computer looks at the whole group. There are 3 pizza lovers (1) and 1 hater (0). This is a pretty mixed group. *   Computation: The Gini Impurity for a set S with C classes is defined as:</p> <pre><code>$$\n\\mathrm{Gini}(S) = 1 - \\sum_{k=1}^{C} p_k^2\n$$\n\nwhere \\(p_k\\) is the proportion of samples in S that belong to class \\(k\\).\n\nIn our dataset (two classes: pizza=1, not-pizza=0):\n\\[\np_1 = \\frac{3}{4}, \\quad p_0 = \\frac{1}{4}.\n\\]\n\nSubstitute and compute:\n\\[\n\\begin{aligned}\n\\mathrm{Gini}(S)\n  &amp;= 1 - \\left(\\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 \\right) \\\\\n  &amp;= 1 - \\left(\\frac{9}{16} + \\frac{1}{16}\\right) \\\\\n  &amp;= 1 - \\frac{10}{16} = \\frac{6}{16} = 0.375.\n\\end{aligned}\n\\]\n\nInterpretation:\n* 0 means perfectly \"pure\" (all samples same class).\n* For binary classification, the maximum Gini is \\(1 - \\tfrac{1}{2} = 0.5\\) (when classes are equally mixed).\n* Another useful interpretation: \\(\\sum_k p_k^2\\) is the probability that two randomly selected samples from S have the same label; therefore \\(\\mathrm{Gini}(S)\\) equals the probability two randomly chosen samples have different labels, i.e., how \"impure\" the set is.\n</code></pre> <ul> <li>Output: The starting \"impurity\" is 0.375. The goal is to ask a question that lowers this number as much as possible.</li> </ul> <pre><code>graph TD\n    subgraph Starting Group\n        A[\"[1,1,1], [0,1,1], [1,0,1], [0,0,0]\"]\n    end\n    B[\"Gini = 0.375\"]\n    A --&gt; B</code></pre> <p>Step 2: Test the First Question (\"Likes Video Games?\") The computer pretends to split the group based on this question.</p> <p><pre><code>graph TD\n    subgraph \"Split on 'Likes Video Games?'\"\n        A[\"Parent Gini = 0.375\"] --&gt; B{Likes Games?};\n        B --&gt;|Yes| C[\"Left Group: [1,1,1], [1,0,1]&lt;br&gt;2 Likes Pizza, 0 Hates&lt;br&gt;&lt;b&gt;Gini = 0.0 (Perfectly Pure!)&lt;/b&gt;\"];\n        B --&gt;|No| D[\"Right Group: [0,1,1], [0,0,0]&lt;br&gt;1 Likes Pizza, 1 Hates&lt;br&gt;&lt;b&gt;Gini = 0.5 (Totally Mixed!)&lt;/b&gt;\"];\n    end</code></pre> *   Computation: For a split, we compute the weighted impurity and the information gain. Formally, with \\(N\\) total samples, \\(N_l\\) left and \\(N_r\\) right:</p> <pre><code>$$\n\\mathrm{Gini}_{\\text{split}} = \\frac{N_l}{N}\\,\\mathrm{Gini}(S_l) + \\frac{N_r}{N}\\,\\mathrm{Gini}(S_r),\n$$\n$$\n\\mathrm{Information\\ Gain} = \\mathrm{Gini}(S) - \\mathrm{Gini}_{\\text{split}}.\n$$\n\nIn our split \"Likes Video Games?\":\n* Left group (Yes): 2 samples, both like pizza \\(\\Rightarrow\\) \\(\\mathrm{Gini}(S_l) = 0\\) (pure).\n* Right group (No): 2 samples, 1 likes, 1 does not \\(\\Rightarrow\\) \\(p=1/2,\\,\\mathrm{Gini}(S_r) = 1 - (1/2)^2 - (1/2)^2 = 0.5\\).\n* Weighted impurity:\n  \\[\n  \\mathrm{Gini}_{\\text{split}} = \\frac{2}{4}\\cdot 0 + \\frac{2}{4}\\cdot 0.5 = 0.25.\n  \\]\n* Information Gain:\n  \\[\n  \\mathrm{Information\\ Gain} = 0.375 - 0.25 = 0.125.\n  \\]\n</code></pre> <ul> <li>Output: The score for this question is \\(0.125\\).</li> </ul> <p>Step 3: Test the Second Question (\"Likes Superheroes?\") Now the computer \"forgets\" the last split and tries again with the next question.</p> <p><pre><code>graph TD\n    subgraph \"Split on 'Likes Superheroes?'\"\n        A[\"Parent Gini = 0.375\"] --&gt; B{Likes Heroes?};\n        B --&gt;|Yes| C[\"Left Group: [1,1,1], [0,1,1]&lt;br&gt;2 Likes Pizza, 0 Hates&lt;br&gt;&lt;b&gt;Gini = 0.0 (Perfectly Pure!)&lt;/b&gt;\"];\n        B --&gt;|No| D[\"Right Group: [1,0,1], [0,0,0]&lt;br&gt;1 Likes Pizza, 1 Hates&lt;br&gt;&lt;b&gt;Gini = 0.5 (Totally Mixed!)&lt;/b&gt;\"];\n    end</code></pre> *   Computation: This yields the same numeric result by symmetry, so its information gain is also \\(0.125\\).</p> <ul> <li>Output: The score for this question is \\(0.125\\).</li> </ul> <p>Step 4: The Decision The computer compares the scores. In this case, they're tied! In a tie, the computer just picks the first one it tried. *   The Winner: The best first question is \"Likes Video Games?\".</p> <p>Step 5: The Recursion The computer now has the root of its tree! It then repeats this entire process on the smaller groups. 1.  It runs the <code>get_best_split</code> logic on the \"Yes\" group. 2.  It runs the <code>get_best_split</code> logic on the \"No\" group. This is the \"endless mirror\" of recursion in action! It continues until the groups are pure.</p> <p>\ud83e\udde0 How Experts Think</p> <p>This process of finding the best question is similar to how an expert, like a doctor, diagnoses an illness. They don't ask random questions. They ask the one question that gives them the most information to narrow down the possibilities. A doctor who asks, \"Do you have a fever?\" is making a \"split\" that provides a high \"information gain.\" The AI is learning to think like an expert!</p>"},{"location":"lessons/lesson6/#the-full-code-a-real-decision-tree","title":"The Full Code: A Real Decision Tree","text":"<p>This code puts all those ideas together. It's complex, but now you can see the \"thinking\" that happens inside it.</p> <pre><code>import numpy as np\n\n# A class to represent a single node in our decision tree\nclass Node:\n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.info_gain = info_gain\n        self.value = value\n\n# The main Decision Tree class\nclass MyDecisionTree:\n    def __init__(self, min_samples_split=2, max_depth=2):\n        self.root = None\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n\n    def build_tree(self, dataset, curr_depth=0):\n        X, Y = dataset[:,:-1], dataset[:,-1]\n        num_samples, num_features = np.shape(X)\n\n        if num_samples &gt;= self.min_samples_split and curr_depth &lt;= self.max_depth:\n            best_split = self.get_best_split(dataset, num_samples, num_features)\n            if best_split[\"info_gain\"] &gt; 0:\n                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n                            left_subtree, right_subtree, best_split[\"info_gain\"])\n\n        leaf_value = self.calculate_leaf_value(Y)\n        return Node(value=leaf_value)\n\n    def get_best_split(self, dataset, num_samples, num_features):\n        best_split = {}\n        max_info_gain = -float(\"inf\")\n\n        for feature_index in range(num_features):\n            feature_values = dataset[:, feature_index]\n            possible_thresholds = np.unique(feature_values)\n            for threshold in possible_thresholds:\n                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n                if len(dataset_left) &gt; 0 and len(dataset_right) &gt; 0:\n                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n                    curr_info_gain = self.information_gain(y, left_y, right_y)\n                    if curr_info_gain &gt; max_info_gain:\n                        best_split[\"feature_index\"] = feature_index\n                        best_split[\"threshold\"] = threshold\n                        best_split[\"dataset_left\"] = dataset_left\n                        best_split[\"dataset_right\"] = dataset_right\n                        best_split[\"info_gain\"] = curr_info_gain\n                        max_info_gain = curr_info_gain\n        return best_split\n\n    def split(self, dataset, feature_index, threshold):\n        dataset_left = np.array([row for row in dataset if row[feature_index] &lt;= threshold])\n        dataset_right = np.array([row for row in dataset if row[feature_index] &gt; threshold])\n        return dataset_left, dataset_right\n\n    def information_gain(self, parent, l_child, r_child):\n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        gain = self.gini_impurity(parent) - (weight_l * self.gini_impurity(l_child) + weight_r * self.gini_impurity(r_child))\n        return gain\n\n    def gini_impurity(self, y):\n        class_labels = np.unique(y)\n        gini = 0\n        for cls in class_labels:\n            p_cls = len(y[y == cls]) / len(y)\n            gini += p_cls**2\n        return 1 - gini\n\n    def calculate_leaf_value(self, Y):\n        Y = list(Y)\n        return max(Y, key=Y.count)\n\n    def fit(self, X, Y):\n        dataset = np.concatenate((X, Y), axis=1)\n        self.root = self.build_tree(dataset)\n\n    def predict(self, X):\n        predictions = [self.make_prediction(x, self.root) for x in X]\n        return predictions\n\n    def make_prediction(self, x, tree):\n        if tree.value != None: return tree.value\n        feature_val = x[tree.feature_index]\n        if feature_val &lt;= tree.threshold:\n            return self.make_prediction(x, tree.left)\n        else:\n            return self.make_prediction(x, tree.right)\n\n# Let's use it!\nX = np.array([[1, 1], [0, 1], [1, 0], [0, 0]])\nY = np.array([[1], [1], [1], [0]])\n\nmy_tree = MyDecisionTree(min_samples_split=2, max_depth=2)\nmy_tree.fit(X, Y)\n\nfrank = [[1, 0]]\nprediction = my_tree.predict(frank)\nprint(\"Our REAL hand-built model predicts Frank's result is:\", prediction)\n</code></pre>"},{"location":"lessons/lesson6/#part-6-lets-discuss","title":"Part 6: Let's Discuss!","text":"<ol> <li>What do you think is the biggest advantage of using a library like <code>scikit-learn</code> now that you've seen the complexity behind it?</li> <li>Our Gini Impurity calculation was simple for our small dataset. How would this process change for a dataset with millions of rows?</li> <li>Does seeing the \"from scratch\" version make the <code>scikit-learn</code> code from Lesson 5 make more sense?</li> </ol> <p>What's Next?</p> <p>This is the end of our introductory journey, but it's just the beginning of your adventure in AI.</p> <p>You have learned: *   What AI, ML, and DL are. *   The different ways machines can learn. *   How to prepare data for a computer. *   How to build a model with a powerful toolbox. *   And now, you've even built a real learning algorithm from scratch!</p> <p>You have a stronger foundation in machine learning than most people on the planet. The next step is to keep building. Keep asking questions. And most importantly, stay curious!</p>"},{"location":"lessons/lesson7/","title":"Lesson 7: Is Our AI Smart? - Grading Your Model \ud83d\udcdd","text":"<p>Welcome back, AI builder!</p> <p>In our last two lessons, we did something amazing: we built a Decision Tree model, both with a powerful toolbox and from scratch! We have a working model. But that leads to the most important question a data scientist can ask: Is our model actually any good?</p> <p>Today, we're going to learn how to be a good scientist and grade our own model. We'll find out if our AI is a real genius or just a good guesser.</p>"},{"location":"lessons/lesson7/#part-1-the-problem-with-cheating","title":"Part 1: The Problem with \"Cheating\"","text":"<p>Imagine you're studying for a big history test. Your teacher gives you a study guide with 20 questions on it. You study those 20 questions until you know them by heart.</p> <p>Then, you get to the test, and the teacher gives you the exact same 20 questions from the study guide. You'd get 100%, right? But did you actually learn history, or did you just memorize the answers to those specific questions?</p> <p>This is the problem we have in machine learning. If we test our model on the same data it trained on, it might get a perfect score. But that doesn't mean it has learned the real patterns. It might have just memorized the \"answers\" in the training data.</p>"},{"location":"lessons/lesson7/#part-2-the-solution-the-test-set","title":"Part 2: The Solution - The \"Test Set\"","text":"<p>To solve this, we do something very simple and very powerful: we split our data into two piles before we start.</p> <ol> <li>The Training Set (The Study Guide): This is the bigger pile, usually about 80% of our data. We give this to our model to learn from. This is its homework, its notes, its study guide.</li> <li>The Test Set (The Final Exam): This is the smaller pile, the other 20%. We hide this data from the model while it's training. The model never, ever gets to see the answers for the test set.</li> </ol> <p>Once the model is fully trained on the training set, we bring out the test set and use it to give the model its final grade. This gives us an honest look at how well our model can perform on new data it has never seen before.</p>"},{"location":"lessons/lesson7/#part-3-the-overfitting-trap","title":"Part 3: The \"Overfitting\" Trap","text":"<p>The \"cheating\" problem we talked about has a fancy name in machine learning: overfitting.</p> <p>Overfitting happens when a model doesn't learn the general patterns in the data, but instead starts to memorize the noisy, random details of the training set. It's like a student who memorizes that \"Question 3's answer is B\" instead of learning the actual history behind the question.</p> <p>An overfit model will look like a genius on the training data, but it will fail miserably on the test data.</p> <pre><code>graph TD\n    subgraph \"A Good Model (Generalizes Well)\"\n        A[Training Data] --&gt; B{Model};\n        B --&gt; C[Gets 92% on Training Set];\n        B --&gt; D[Gets 90% on Test Set];\n    end\n\n    subgraph \"An Overfit Model (Memorized)\"\n        E[Training Data] --&gt; F{Model};\n        F --&gt; G[Gets 100% on Training Set!];\n        F --&gt; H[Gets 55% on Test Set... Oops!];\n    end</code></pre> <p>A good model might not get a perfect score on the training data, but its score on the test data will be very close. That's how we know it has learned the real patterns!</p>"},{"location":"lessons/lesson7/#part-4-the-code-splitting-and-scoring","title":"Part 4: The Code - Splitting and Scoring","text":"<p>So how do we do this in code? <code>scikit-learn</code> makes it incredibly easy!</p> <ol> <li><code>train_test_split</code>: This function automatically shuffles and splits our data into training and testing sets.</li> <li><code>.score()</code>: After training, we can use this method on our model to see its grade (its accuracy) on the test set.</li> </ol> <p>Let's see it in action:</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split # Our new tool!\n\n# Step 1: Prepare the data\n# Let's imagine we have more data this time\nX = [[1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 0]]\ny = [1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n\n# Step 2: Split the data into a training set and a test set\n# test_size=0.2 means we'll save 20% of the data for the final exam\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Create and train the model (ONLY on the training set!)\nmy_model = DecisionTreeClassifier()\nmy_model.fit(X_train, y_train)\n\n# Step 4: Grade the model on the unseen test set\naccuracy = my_model.score(X_test, y_test)\n\nprint(\"The model's accuracy on the final exam is:\", accuracy)\n\n# The output might be: The model's accuracy on the final exam is: 1.0 (which is 100%!)\n# In this case, our model got a perfect score on the two questions in our test set.\n</code></pre>"},{"location":"lessons/lesson7/#part-5-is-accuracy-always-the-best-grade","title":"Part 5: Is Accuracy Always the Best Grade?","text":"<p>For our pizza problem, getting a grade of 90% (an accuracy of <code>0.9</code>) sounds great! But is accuracy always the best way to grade a model?</p> <p>The Analogy: The Rare Disease Detector Imagine you build an AI to detect a very rare disease that only 1 out of 100 people has.</p> <p>A lazy, useless model could just predict \"no disease\" for every single person. Think about its accuracy. It would be correct for the 99 people who don't have the disease, and wrong for the 1 person who does. Its accuracy would be 99%!</p> <p>It got an A+ grade, but it's a terrible model because it never finds the one person who actually needs help.</p> <p>In cases like this, scientists use other metrics like Precision (of all the patients the model said were sick, how many actually were?) and Recall (of all the patients who were really sick, how many did the model find?).</p> <p>\ud83e\udde0 Different Kinds of \"Correct\"</p> <p>This is true in life, too! Getting a 99% on a test is great. But if a smoke detector works 99% of the time, that 1% of the time it fails could be disastrous. Sometimes, certain kinds of mistakes are much worse than others. Grading an AI is about understanding which mistakes are the most important to avoid.</p>"},{"location":"lessons/lesson7/#part-6-lets-discuss","title":"Part 6: Let's Discuss!","text":"<ol> <li>Why should you never, ever train your model on your test data?</li> <li>If a model gets 100% on the training set but only 50% on the test set, what problem is it suffering from?</li> <li>Can you think of another example (besides the disease detector) where a high accuracy score might be misleading?</li> </ol> <p>What's Next?</p> <p>This is the end of our introductory series, and you have been an amazing student!</p> <p>You have learned: *   What AI, ML, and DL are. *   The different ways machines can learn. *   How to prepare data for a computer. *   How to build a model with a powerful toolbox. *   How to peek under the hood to see how it all works. *   And now, how to properly grade your work like a real scientist.</p> <p>You have a stronger foundation in machine learning than most people on the planet. The journey from here is endless and exciting. You can explore new models, work with bigger and more interesting datasets, and start building projects that can help people.</p> <p>The most important thing is to never stop learning. Stay curious!</p>"},{"location":"lessons/lesson8/","title":"Lesson 8: You Are Who Your Friends Are - K-Nearest Neighbors \ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1","text":"<p>Welcome back, AI explorer!</p> <p>You've learned how to build and evaluate a Decision Tree, which is a model that learns by creating a set of <code>if/else</code> rules. But that's not the only way a machine can learn!</p> <p>Today, we're going to learn about a completely different and wonderfully simple algorithm called K-Nearest Neighbors (KNN). Its core idea is something you already understand from everyday life: you can tell a lot about someone by the company they keep.</p>"},{"location":"lessons/lesson8/#part-1-the-big-idea-the-friend-group-analogy","title":"Part 1: The Big Idea (The \"Friend Group\" Analogy)","text":"<p>Imagine it's your first day at a new school. You see a student you don't know, and you want to guess if they like playing soccer. You look around and see them talking and laughing with three other students. You know all three of those students are on the school's soccer team.</p> <p>What's your prediction? You'd probably guess the new student likes soccer, too!</p> <p>You just used the K-Nearest Neighbors algorithm. The logic is simple: to predict something about a new data point, first find its \"K\" closest neighbors, then have them vote.</p>"},{"location":"lessons/lesson8/#part-2-finding-the-nearest-neighbors","title":"Part 2: Finding the \"Nearest\" Neighbors","text":"<p>For a computer, \"closest\" doesn't mean sitting next to someone at lunch. It means the distance on a graph. Let's imagine we have a graph where one axis is \"Likes Video Games\" and the other is \"Likes Superheroes.\" We can plot all our students on this graph.</p> <p>Now, a new student, \"Frank,\" comes along. We can plot him on the graph, too. To find his nearest neighbors, the computer simply measures the distance to all the other points.</p> <p>Let's say we choose K=3. The algorithm will find the 3 students who are closest to Frank on the graph.</p> <p><pre><code>graph TD\n    subgraph \"Finding Frank's 3 Nearest Neighbors (K=3)\"\n        A[\"Frank (Unknown)\"] -- closest --&gt; B(\"Alex - Pizza Lover\");\n        A -- 2nd closest --&gt; C(\"Chloe - Pizza Lover\");\n        A -- 3rd closest --&gt; D(Ben - Pizza Lover);\n        E(David - Not a Pizza Lover) -.-&gt; A;\n    end</code></pre> In this case, Frank's three closest \"friends\" on the graph are Alex, Chloe, and Ben. David is too \"far away\" to be in his group of K=3.</p>"},{"location":"lessons/lesson8/#part-3-making-a-prediction-majority-vote","title":"Part 3: Making a Prediction (Majority Vote)","text":"<p>Once we have our K-nearest neighbors, making a prediction for classification is easy: it's a majority vote!</p> <ol> <li>Look at the neighbors' labels:<ul> <li>Alex: Likes Pizza</li> <li>Chloe: Likes Pizza</li> <li>Ben: Likes Pizza</li> </ul> </li> <li>Count the votes: That's 3 votes for \"Likes Pizza\" and 0 votes for \"Doesn't Like Pizza.\"</li> <li>Make the prediction: The majority wins! We predict that Frank will like pizza.</li> </ol> <p>It's that intuitive!</p> <p>\ud83e\udde0 The Power of Social Influence</p> <p>KNN is a mathematical version of a powerful psychological principle called \"social proof.\" We often decide what to do or what to like based on the people around us. If you see a long line outside an ice cream shop, you assume the ice cream must be good. The KNN algorithm uses the same logic: if all the \"neighboring\" data points have a certain label, the new point probably has that label too.</p>"},{"location":"lessons/lesson8/#part-4-what-about-regression-averaging-the-friends","title":"Part 4: What About Regression? (Averaging the Friends)","text":"<p>KNN is amazing because it can do regression, too! But instead of a vote, it takes an average.</p> <p>Let's say we want to predict a new student's score on a math test. 1.  We find the K=3 closest neighbors (the students who have similar study habits, maybe). 2.  We look at their test scores: Student A got a 90, Student B got a 95, and Student C got an 88. 3.  Instead of voting, we average their scores: <code>(90 + 95 + 88) / 3 = 91</code>. 4.  Our prediction for the new student's test score is a 91!</p> <p>So, for KNN: *   Classification = Majority Vote *   Regression = Average</p>"},{"location":"lessons/lesson8/#part-5-the-code-knn-in-scikit-learn","title":"Part 5: The Code - KNN in Scikit-learn","text":"<p>Here's the best part. Because <code>scikit-learn</code> is so well-designed, swapping our Decision Tree for a KNN model is incredibly easy. The code is almost identical!</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier # We just import a different tool!\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Prepare the data (same as before)\nX = [[1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 0]]\ny = [1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n\n# Step 2: Split the data (same as before)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Create and train the model\n# Instead of DecisionTreeClassifier, we use KNeighborsClassifier!\n# We'll tell it to use 3 neighbors (K=3).\nmy_model = KNeighborsClassifier(n_neighbors=3)\nmy_model.fit(X_train, y_train) # The .fit() method works the same way!\n\n# Step 4: Grade the model (same as before)\naccuracy = my_model.score(X_test, y_test)\n\nprint(\"The KNN model's accuracy on the final exam is:\", accuracy)\n</code></pre> <p>This shows the power of a good library. Once you know the workflow (split, fit, score), you can easily experiment with different types of models!</p>"},{"location":"lessons/lesson8/#part-6-lets-discuss","title":"Part 6: Let's Discuss!","text":"<ol> <li>In our example, we used K=3. What might happen if we set K=1? What could be a potential problem with that?</li> <li>What might happen if our dataset had 100 students and we set K=100?</li> <li>Can you think of a time in real life when you used \"nearest neighbors\" logic to make a decision or a guess about something?</li> </ol> <p>What's Next?</p> <p>Congratulations! You've added another powerful algorithm to your machine learning toolkit. You're seeing that there are many different ways to teach a machine, each with its own logic.</p> <p>This lesson concludes our initial journey into the world of specific ML algorithms. From here, the world is your oyster. You can explore more complex models like Neural Networks, dive deeper into Deep Learning, or start working on your own projects with real-world data.</p> <p>You have learned an incredible amount. Never stop being curious, and never stop building!</p>"},{"location":"lessons/lesson9/","title":"Lesson 9: The Friendship Bracelet - Building KNN From Scratch \ud83e\uddf5","text":"<p>Welcome back, master builder!</p> <p>You've seen the power of <code>scikit-learn</code>, but to truly understand an algorithm, there's nothing like building it yourself. Today, we're going to build the K-Nearest Neighbors algorithm from scratch using nothing but pure Python. No libraries, no hidden magic.</p> <p>This will be a challenge, but by the end, you'll have a deep, powerful understanding of how a computer can \"learn\" by looking at its neighbors.</p>"},{"location":"lessons/lesson9/#part-1-the-plan-our-recipe","title":"Part 1: The Plan (Our Recipe)","text":"<p>Every good project starts with a plan. Here's our four-step recipe for building a KNN model:</p> <ol> <li>Measure Friendship: For our new data point, we'll calculate the \"distance\" to every single data point in our training set.</li> <li>Find the Inner Circle: We'll sort all those distances and find the \"K\" closest neighbors.</li> <li>Ask the Friends: We'll look at the labels of those K neighbors.</li> <li>Make a Prediction: Based on what the neighbors say, we'll make our final prediction.</li> </ol>"},{"location":"lessons/lesson9/#part-2-measuring-friendship-the-distance-formula","title":"Part 2: Measuring Friendship (The Distance Formula)","text":"<p>How do we measure \"distance\" between data points? The most common way is called Euclidean Distance.</p> <p>The Analogy: Imagine two points on a treasure map. The Euclidean distance is the \"as the crow flies\" straight-line distance between them.</p> <p>If we have two students, Student 1 (\\(p_1\\)) and Student 2 (\\(p_2\\)), with features \\((x_1, y_1)\\) and \\((x_2, y_2)\\), the formula is:</p> \\[ d(p_1, p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\] <p>It looks a bit scary, but it's just what we're doing in the code below!</p> <p>Here's how we can write a <code>euclidean_distance</code> function in pure Python:</p> <pre><code>def euclidean_distance(point1, point2):\n    \"\"\"Calculates the straight-line distance between two points.\"\"\"\n    sum_of_squares = 0\n    # Loop through each feature (e.g., 'Likes Games', 'Likes Heroes')\n    for i in range(len(point1)):\n        sum_of_squares += (point1[i] - point2[i]) ** 2\n\n    # The ** 0.5 is the same as doing a square root!\n    return sum_of_squares ** 0.5\n\n# --- Example Calculation ---\n# Our training data has a student, Alex, at [1, 1]\n# Our new student, Frank, is at [1, 0]\nalex = [1, 1]\nfrank = [1, 0]\n\n# Input: Two points\ndistance = euclidean_distance(alex, frank)\n\n# Output: The distance\nprint(\"The distance between Alex and Frank is:\", distance) \n# Expected Output: 1.0\n</code></pre>"},{"location":"lessons/lesson9/#part-3-the-code-step-by-step","title":"Part 3: The Code, Step-by-Step","text":"<p>Now let's build our algorithm, one function at a time.</p>"},{"location":"lessons/lesson9/#step-1-get-the-neighbors","title":"Step 1: Get the Neighbors","text":"<p>This function will take our training data, a new student, and our choice of \"K\", and it will find the K closest friends.</p> <pre><code>def get_neighbors(train_data, new_student, k):\n    \"\"\"Finds the K nearest neighbors.\"\"\"\n    distances = []\n    for train_student in train_data:\n        # Separate the features from the label\n        features = train_student[:-1]\n        label = train_student[-1]\n\n        # Calculate the distance\n        dist = euclidean_distance(new_student, features)\n        distances.append((dist, label))\n\n    # Sort the list of neighbors by their distance\n    distances.sort(key=lambda tup: tup[0])\n\n    # Get just the top K neighbors\n    neighbors = []\n    for i in range(k):\n        neighbors.append(distances[i][1])\n\n    return neighbors\n</code></pre>"},{"location":"lessons/lesson9/#step-2a-predict-for-classification-majority-vote","title":"Step 2A: Predict for Classification (Majority Vote)","text":"<p>This function takes the list of neighbors and finds the most common label.</p> <pre><code>def predict_classification(neighbors):\n    \"\"\"Predicts the class based on a majority vote.\"\"\"\n    votes = {}\n    for neighbor_label in neighbors:\n        if neighbor_label in votes:\n            votes[neighbor_label] += 1\n        else:\n            votes[neighbor_label] = 1\n\n    # Find the label with the most votes\n    # The `sorted` function here is a clever way to get the max value from a dictionary\n    prediction = sorted(votes.items(), key=lambda item: item[1], reverse=True)\n    return prediction[0][0]\n</code></pre> <p>\ud83e\udd14 The \"Even K\" Problem: What happens if you choose K=2 and one neighbor is a \"Pizza Lover\" and the other is a \"Pizza Hater\"? It's a tie! This is why for classification problems, data scientists almost always choose an odd number for K (like 1, 3, 5) to avoid ties.</p>"},{"location":"lessons/lesson9/#step-2b-predict-for-regression-the-average","title":"Step 2B: Predict for Regression (The Average)","text":"<p>This function takes the list of neighbors and finds their average value.</p> <pre><code>def predict_regression(neighbors):\n    \"\"\"Predicts a number based on the average of the neighbors.\"\"\"\n    total = 0\n    for neighbor_label in neighbors:\n        total += neighbor_label\n\n    return total / len(neighbors)\n</code></pre>"},{"location":"lessons/lesson9/#part-4-putting-it-all-together","title":"Part 4: Putting It All Together","text":"<p>Now we can combine our functions to build a complete, from-scratch model!</p>"},{"location":"lessons/lesson9/#example-1-the-pizza-classifier","title":"Example 1: The Pizza Classifier","text":"<pre><code># Our training data, with the label at the end\npizza_data = [\n    [1, 1, 1],  # Alex: Likes Games, Likes Heroes, Likes Pizza\n    [0, 1, 1],  # Ben\n    [1, 0, 1],  # Chloe\n    [0, 0, 0]   # David\n]\n\n# Our new student, Frank\nfrank_features = [1, 0]\n\n# --- Let's Predict! ---\n# 1. Get the 3 nearest neighbors\nk = 3\nfrank_neighbors = get_neighbors(pizza_data, frank_features, k)\nprint(\"Frank's 3 closest neighbors have labels:\", frank_neighbors)\n\n# 2. Make a prediction\nprediction = predict_classification(frank_neighbors)\nprint(\"So, we predict Frank's result is:\", prediction)\n\n# Expected Output:\n# Frank's 3 closest neighbors have labels: [1, 1, 1]\n# So, we predict Frank's result is: 1 (Likes Pizza)\n</code></pre>"},{"location":"lessons/lesson9/#example-2-the-test-score-regressor","title":"Example 2: The Test Score Regressor","text":"<pre><code># Data: [Hours Studied, Past Score, Current Score]\nscore_data = [\n    [2, 80, 85],\n    [4, 90, 92],\n    [1, 75, 78],\n    [5, 95, 98]\n]\n\n# A new student who studied for 3 hours and got an 85 on the last test\nnew_student_features = [3, 85]\n\n# --- Let's Predict! ---\n# 1. Get the 2 nearest neighbors\nk = 2\nnew_student_neighbors = get_neighbors(score_data, new_student_features, k)\nprint(\"The new student's 2 closest neighbors have scores:\", new_student_neighbors)\n\n# 2. Make a prediction\nprediction = predict_regression(new_student_neighbors)\nprint(\"So, we predict the new student's score will be:\", prediction)\n\n# Expected Output:\n# The new student's 2 closest neighbors have scores: [85, 92]\n# So, we predict the new student's score will be: 88.5\n</code></pre>"},{"location":"lessons/lesson9/#part-5-lets-discuss","title":"Part 5: Let's Discuss!","text":"<ol> <li>Now that you've built it, what do you think is the \"hardest\" part of the KNN algorithm for the computer to do?</li> <li>How would our <code>euclidean_distance</code> function need to change if we had three features instead of two?</li> <li>Why was it important that we built two different prediction functions (<code>predict_classification</code> and <code>predict_regression</code>)?</li> </ol> <p>What's Next?</p> <p>This is a huge accomplishment! You have built a complete, working machine learning algorithm from the ground up using nothing but basic Python. You are no longer just a user of AI tools\u2014you are a creator.</p> <p>This concludes our introductory series. You have journeyed from the highest-level ideas of AI to the nitty-gritty details of writing your own learning algorithms. The world of AI is now open to you.</p> <p>Keep experimenting, keep building, and most importantly, stay curious!</p>"}]}