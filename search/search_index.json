{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Welcome to SimplestLearn Wiki!","text":"<p>Welcome to your journey into the world of Artificial Intelligence, Machine Learning, and Deep Learning!</p> <p>This wiki is designed to teach you the fundamentals of AI/ML in the simplest, most intuitive way possible. No complex math. No boring jargon. Just clear explanations and real-world examples.</p>"},{"location":"#what-youll-learn","title":"\ud83d\udcda What You'll Learn","text":"<p>Our curriculum is organized into foundational lessons that build on each other:</p>"},{"location":"#lesson-1-introduction-to-aimldl","title":"Lesson 1: Introduction to AI/ML/DL \ud83e\udd16","text":"<p>Learn what AI really is and how it's organized into different \"dolls\" - from Artificial Intelligence down to Deep Learning. Perfect for understanding the big picture!</p>"},{"location":"#lesson-2-the-three-flavors-of-machine-learning","title":"Lesson 2: The Three Flavors of Machine Learning \ud83c\udf66","text":"<p>Discover the three main types of machine learning: Supervised Learning (learning with answers), Unsupervised Learning (finding patterns), and Reinforcement Learning (learning by doing).</p>"},{"location":"#lesson-3-classification-vs-regression","title":"Lesson 3: Classification vs Regression \ud83c\udfaf","text":"<p>Dive deep into Supervised Learning and learn about the two main problems ML solves: sorting things into categories (classification) or predicting numbers (regression).</p>"},{"location":"#lesson-4-getting-your-data-ready","title":"Lesson 4: Getting Your Data Ready \ud83c\udf73","text":"<p>Learn the most important step in any AI project: preparing your data! We'll cover how to turn words into numbers (encoding) and handle real-world problems like missing data.</p>"},{"location":"#lesson-5-your-first-ai-model","title":"Lesson 5: Your First AI Model \ud83c\udf33","text":"<p>Time to code! We'll use the powerful <code>scikit-learn</code> library to build our first real machine learning model, a Decision Tree, in just a few lines of Python.</p>"},{"location":"#lesson-6-under-the-hood","title":"Lesson 6: Under the Hood \ud83d\udee0\ufe0f","text":"<p>For the truly curious, we'll pull back the curtain and build a learning Decision Tree algorithm from scratch to see how the magic really works.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Start with Lesson 1 to understand the fundamentals, then progress through the lessons in order. Each one builds on the previous, taking you from a beginner to a true AI builder!</p> <p>Happy learning! Remember: everyone was a beginner once. You've got this! \ud83d\udcaa</p>"},{"location":"lessons/lesson1/","title":"Lesson 1: Your Secret Guide to Artificial Intelligence! \ud83e\udd16","text":"<p>Hey there, future tech genius! </p> <p>Ever wondered how YouTube seems to know exactly what video you want to watch next? Or how Snapchat puts those funny filters on your face? It's not magic, it's something even cooler: Artificial Intelligence, or AI for short.</p> <p>Welcome to your very first lesson where we'll uncover the secrets of AI, Machine Learning (ML), and Deep Learning (DL). We're going to skip the super-boring math and get straight to the fun, powerful ideas that are changing the world.</p>"},{"location":"lessons/lesson1/#part-1-the-big-picture-what-is-this-ai-magic","title":"Part 1: The Big Picture - What is this AI Magic?","text":"<p>Imagine you have a set of Russian nesting dolls. You know, those wooden dolls that have smaller dolls inside them? AI, ML, and DL are kind of like that!</p> <ul> <li> <p>Artificial Intelligence (AI) is the biggest doll. It's the whole grand idea of making machines smart. This could be anything from a chess-playing computer to a robot that can have a conversation with you. If a machine is doing something that would normally require a human brain\u2014like learning, reasoning, or problem-solving\u2014that's AI!</p> </li> <li> <p>Machine Learning (ML) is the next doll inside. This is where things get really interesting. Instead of giving the machine a giant list of rules for every single situation, we let it learn on its own! We feed it a ton of information, which we call data, and it starts to figure out the patterns. It's like how you learn to ride a bike. You don't read a manual; you just keep trying, and your brain figures it out. For a computer, data is its experience!</p> </li> <li> <p>Deep Learning (DL) is the smallest, most powerful doll. This is a super-special kind of machine learning that's inspired by the human brain. It's amazing at finding very tricky patterns in huge amounts of data. It's the secret sauce behind things like self-driving cars recognizing pedestrians, or voice assistants like Siri and Alexa understanding what you say.</p> </li> </ul> <p>Here's a little diagram to help you remember:</p> <pre><code>graph TD\n    A[Artificial Intelligence] --&gt; B[Machine Learning];\n    B --&gt; C[Deep Learning];</code></pre> <p>Real-World Examples:</p> <ul> <li> <p>AI: A character in a video game that cleverly finds its way around obstacles to reach you.</p> </li> <li> <p>ML: Your email inbox automatically sorting emails into \"spam\" and \"not spam\" based on thousands of examples it has seen before.</p> </li> <li> <p>DL: A website that can look at a picture of a dog and tell you what breed it is, even if it's never seen that specific dog before!</p> </li> </ul>"},{"location":"lessons/lesson1/#part-2-a-blast-from-the-past-the-secret-history-of-smart-machines","title":"Part 2: A Blast from the Past - The Secret History of Smart Machines","text":"<p>You might think AI is a new thing, but people have been dreaming about smart machines for a long time. Let's hop in our time machine!</p> <pre><code>timeline\n    title The History of AI\n    1950s : The Dream Begins\n    1960s-70s : The \"AI Winter\" \ud83e\udd76\n    1980s : The Comeback Kid\n    1990s-2000s : The Rise of the Machines\n    2010s-Today : The AI Explosion!</code></pre> <p>Story begins here:</p> <ul> <li> <p>1950s : The Dream Begins</p> <ul> <li>Alan Turing, a hero who helped crack secret codes in World War II (enigma machine), asks the big question: \"Can machines think?\"</li> <li>The term \"Artificial Intelligence\" is born at a summer workshop where a bunch of brilliant scientists decided to make it a real field of study.</li> </ul> </li> <li> <p>1960s-70s : The \"AI Winter\" \ud83e\udd76</p> <ul> <li>Early excitement was huge! Researchers made bold promises, but computers back then were as powerful as a modern calculator. They just couldn't handle the big ideas.</li> <li>When the progress was slow, people got disappointed, and funding for research dried up. It was a sad, chilly time for our robot friends.</li> </ul> </li> <li> <p>1980s : The Comeback Kid</p> <ul> <li>A new type of AI called \"expert systems\" gets popular. These were programs designed to be experts in one specific thing, like diagnosing diseases or finding oil.</li> </ul> </li> <li> <p>1990s-2000s : The Rise of the Machines (in a good way!)</p> <ul> <li>Computers get way, way more powerful.</li> <li>In 1997, a computer named Deep Blue beats the world chess champion, Garry Kasparov! This was a huge moment. \ud83e\udd2f</li> </ul> </li> <li> <p>2010s-Today : The AI Explosion!</p> <ul> <li>Thanks to even more powerful computers and tons of data from the internet, Deep Learning takes over!</li> <li>This leads to the amazing AI we see today: self-driving cars, voice assistants, and AI that can create stunning art and music!</li> </ul> </li> </ul> <p>Funny Story Time: In 1952, a computer scientist named Arthur Samuel created a checkers program that could learn from its own mistakes. After playing thousands of games against itself, it eventually became better than its creator! Arthur Samuel said it was \"a very humbling experience.\" It was one of the first examples of a machine truly learning.</p>"},{"location":"lessons/lesson1/#part-3-your-first-adventure-in-machine-learning-the-case-of-the-pizza-loving-student","title":"Part 3: Your First Adventure in Machine Learning - The Case of the Pizza-Loving Student","text":"<p>Okay, enough history. Let's get our hands dirty with a real machine learning problem!</p> <p>The Mission: We need to build a model that can predict if a new student will like pizza. \ud83c\udf55</p> <p>The Clues (Our Data): We have some information about other students. This is our training data. In machine learning, the clues we use to make a prediction are called features, and the thing we're trying to predict is called the label.</p> Student Likes Video Games? (Feature 1) Likes Superheroes? (Feature 2) Likes Pizza? (Label) Alex Yes Yes Yes Ben No Yes Yes Chloe Yes No Yes David No No No Emily Yes Yes Yes <p>The Detective Work (Building a Model): Now, we need to find a pattern. Looking at the table, what do you notice?</p> <p>It seems like if a student likes video games OR superheroes, they probably like pizza. That's our model! It's a simple rule that our machine can use to make predictions.</p> <p>The Test (Is Our Model Any Good?): A new student, Frank, arrives. He loves video games but isn't a big fan of superheroes.</p> <ul> <li>Our Model: If a student likes video games OR superheroes, they will like pizza.</li> <li>Frank's Features: He likes video games.</li> <li>Prediction: Frank will like pizza!</li> </ul> <p>But wait! Another new student, Grace, arrives. She doesn't like video games or superheroes.</p> <ul> <li>Our Model: If a student likes video games OR superheroes, they will like pizza.</li> <li>Grace's Features: She doesn't like video games or superheroes.</li> <li>Prediction: Grace will not like pizza.</li> </ul> <p>Let's say we ask Grace, and she says she loves pizza! Our model was wrong! And that's one of the most important lessons in machine learning: models are not perfect. They are just our best guess based on the data we have. A big part of machine learning is testing our models and finding ways to make them better.</p>"},{"location":"lessons/lesson1/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<p>Now it's your turn to be the AI expert. Think about these questions:</p> <ol> <li>Can you think of three things you use every day that might use AI? (Hint: think about your phone, your TV, and the internet!)</li> <li>If you could teach a robot to do any chore in your house, what would it be? What kind of \"data\" would you need to give it so it could learn?</li> <li>Our pizza model wasn't perfect. What other \"features\" (clues) could we collect about students to make our pizza predictions better?</li> </ol> <p>What's Next?</p> <p>You've taken a huge step into the amazing world of AI! You've learned the difference between AI, ML, and DL, traveled through history, and even built and tested your very own machine learning model.</p> <p>Next time, we'll explore some of the different types of machine learning and start to see how a little bit of code can bring these ideas to life.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson2/","title":"Lesson 2: The Three Flavors of Machine Learning! \ud83c\udf66","text":"<p>Hey again, future tech genius!</p> <p>Last time, we learned what AI is and built our very first (and very simple) machine learning model to predict pizza lovers. But it turns out, \"learning\" can mean a few different things for a computer.</p> <p>Think of it like ice cream. You might have chocolate, vanilla, and strawberry. They're all ice cream, but they're different flavors. Machine learning is the same! There are three main \"flavors,\" or types, of machine learning. Let's grab a spoon and dig in!</p>"},{"location":"lessons/lesson2/#part-1-supervised-learning-the-copycat","title":"Part 1: Supervised Learning (The Copycat)","text":"<p>This is the most common type of machine learning. Supervised Learning is like studying for a test with flashcards. On one side of the card, you have a question (the data), and on the other side, you have the answer (the label).</p> <p>You show the computer thousands and thousands of these \"flashcards.\" After a while, it learns the relationship between the questions and the answers so well that it can start predicting the answer for new questions it has never seen before.</p> <p>The Big Idea: You supervise the computer by giving it all the correct answers to learn from.</p> <p>Real-World Example: Is it a Cat or a Dog? \ud83d\udc31\ud83d\udc36 Imagine you want to teach a computer to recognize photos of cats and dogs.</p> <ol> <li> <p>You get the data: You collect thousands of pictures of cats and thousands of pictures of dogs.</p> </li> <li> <p>You label it: You go through every picture and label it \"cat\" or \"dog.\" This is the answer key!</p> </li> <li> <p>You train the model: You feed all these labeled pictures to your machine learning model. It studies the pictures, looking for patterns. It might learn that \"cats have pointy ears\" and \"dogs have floppy ears,\" but it will find thousands of patterns that are way more complex.</p> </li> <li> <p>You test it: Now, you show it a brand new picture of a cat it has never seen before. Because it has learned the patterns, it can confidently predict, \"That's a cat!\"</p> </li> </ol> <p>Our pizza-predictor from Lesson 1 was a simple example of supervised learning!</p> <pre><code>graph TD\n    subgraph Supervised Learning [Learning with an Answer Key]\n        direction LR\n        A[Labeled Data &lt;br&gt; e.g., pictures of cats labeled 'cat'] --&gt; B{Model};\n        B --&gt; C[Prediction &lt;br&gt; e.g., 'That's a cat!'];\n    end</code></pre>"},{"location":"lessons/lesson2/#part-2-unsupervised-learning-the-detective","title":"Part 2: Unsupervised Learning (The Detective \ud83d\udd75\ufe0f)","text":"<p>What if you don't have an answer key? What if you just have a giant pile of data and you want the computer to find any interesting patterns or groups on its own? That's Unsupervised Learning!</p> <p>This is like being given a giant box of mixed-up LEGOs and being told to \"sort them.\" You might sort them by color, by size, or by shape. You're finding the structure all by yourself, without any instructions.</p> <p>The Big Idea: The computer is unsupervised. It has to find the hidden patterns and groupings in the data on its own.</p> <p>Real-World Example: Discovering New Music \ud83c\udfb6 Have you ever wondered how Spotify or Apple Music are so good at recommending new songs you might like? They use unsupervised learning!</p> <ol> <li> <p>They get the data: They look at the listening habits of millions of people. What songs do people listen to together? What artists do the same people like?</p> </li> <li> <p>The model finds clusters: The unsupervised learning model dives into this data and starts to group songs together. It might create a cluster of high-energy dance songs, a cluster of calm study music, and a cluster of classic rock songs. It doesn't know they are \"dance\" or \"rock,\" it just knows that people who listen to one song in the cluster tend to listen to the others.</p> </li> <li> <p>You get a recommendation: When you listen to a song from one of those clusters, the service can recommend other songs from that same cluster, thinking you'll probably like them too!</p> </li> </ol> <pre><code>graph TD\n    subgraph Unsupervised Learning [Finding Patterns on Your Own]\n        direction LR\n        D[Unlabeled Data &lt;br&gt; e.g., a mix of songs] --&gt; E{Model};\n        E --&gt; F[Clusters/Groups &lt;br&gt; e.g., 'Songs people listen to together'];\n    end</code></pre>"},{"location":"lessons/lesson2/#part-3-reinforcement-learning-the-pet-trainer","title":"Part 3: Reinforcement Learning (The Pet Trainer \ud83d\udc3e)","text":"<p>This is the coolest and most futuristic type of learning. Reinforcement Learning is all about learning by trial and error, just like training a pet.</p> <p>You put an AI \"agent\" in an environment (like a video game) and give it a goal. When it does something that gets it closer to the goal, you give it a reward (like a treat). When it does something wrong, you might give it a small penalty. Over time, the agent learns to take actions that get it the most rewards.</p> <p>The Big Idea: The agent learns the best actions to take through a system of rewards and punishments, reinforcing good behavior.</p> <p>Real-World Example: A Video Game Pro \ud83c\udfae Imagine an AI learning to play a car racing game.</p> <ol> <li> <p>The Goal: Finish the race as fast as possible.</p> </li> <li> <p>The Environment: The racetrack.</p> </li> <li> <p>The Rewards: The AI gets points for staying on the track, for going fast, and gets a huge bonus for finishing first. It loses points for crashing.</p> </li> <li> <p>The Learning: At first, the AI is terrible! It crashes constantly. But it learns that crashing is bad (negative reward) and staying on the track is good (positive reward). After playing millions of games, it learns the perfect line to take on every turn and becomes an unbeatable racing champion. This is how AI has learned to master complex games like Chess and Go!</p> </li> </ol> <pre><code>graph TD\n    subgraph Reinforcement Learning [Learning from Experience]\n        G{Agent &lt;br&gt; e.g., a game character} -- Takes Action --&gt; H[Environment &lt;br&gt; e.g., the game world];\n        H -- Sends back --&gt; I[Reward or Penalty &lt;br&gt; e.g., +10 points!];\n        I -- Teaches --&gt; G;\n    end</code></pre>"},{"location":"lessons/lesson2/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<ol> <li> <p>Which of the three \"flavors\" of machine learning do you think is the most interesting? Why?</p> </li> <li> <p>If you were to build a spam filter for your email, which type of learning would you use? What would be the \"data\" and the \"labels\"?</p> </li> <li> <p>Can you think of a game where you learned to get better through trial and error (reinforcement learning)?</p> </li> </ol> <p>What's Next?</p> <p>Incredible! You're now an expert on the three main ways machines learn. You've seen how these different \"flavors\" are used all around us, from our photo apps to our music playlists.</p> <p>Next time, we'll start to think about how we can actually write some simple code to make these ideas come to life. Get ready to become a real AI builder!</p> <p>Stay curious!</p>"},{"location":"lessons/lesson3/","title":"Lesson 3: The Two Big Problems ML Can Solve! \ud83c\udfaf","text":"<p>Hello again, super-learner!</p> <p>So far, we've learned what AI is and explored the three \"flavors\" of machine learning. We paid special attention to Supervised Learning, where we teach a computer using an answer key.</p> <p>Today, we're going to zoom in on Supervised Learning and discover the two main types of problems it's really good at solving: Classification and Regression. It sounds fancy, but it's as simple as sorting laundry and guessing numbers!</p>"},{"location":"lessons/lesson3/#part-1-classification-the-sorter","title":"Part 1: Classification (The Sorter \ud83e\uddfa)","text":"<p>Classification is all about teaching a machine to sort things into groups or categories. The answer is always one of a few specific choices.</p> <p>The Analogy: Sorting Laundry Imagine you have a giant pile of laundry. Your job is to sort it into the right baskets: one for whites, one for colors, and one for darks. That's classification! You're assigning each piece of clothing to a specific category.</p> <p>The Big Idea: The model predicts a category or a class.</p> <p>Real-World Examples:</p> <ul> <li> <p>Spam or Not Spam? Your email uses classification to read an incoming email and decide: should it go in the \"spam\" basket or the \"inbox\" basket?</p> </li> <li> <p>Cat vs. Dog: Our example from last lesson was a classic classification problem. The model looks at a picture and sorts it into the \"cat\" category or the \"dog\" category.</p> </li> <li> <p>Pizza Lover?: Our very first model was also a classification model! It predicted whether a student belonged to the \"likes pizza\" group or the \"doesn't like pizza\" group.</p> </li> </ul>"},{"location":"lessons/lesson3/#part-2-regression-the-guesser","title":"Part 2: Regression (The Guesser \ud83d\udd22)","text":"<p>Regression is all about teaching a machine to predict a specific number or value. The answer can be any number within a range.</p> <p>The Analogy: Guessing Jellybeans Imagine a big jar full of jellybeans. If you try to guess exactly how many are inside, you're making a regression guess. Your answer isn't a category like \"a lot\" or \"a little,\" but a specific number, like \"852 jellybeans.\"</p> <p>The Big Idea: The model predicts a numerical value.</p> <p>Real-World Examples:</p> <ul> <li> <p>Weather Forecast: When your weather app says it will be 75\u00b0F tomorrow, that's a regression model at work. It's predicting a specific number on the temperature scale.</p> </li> <li> <p>Pizza Delivery Time: When you order a pizza and the app says it will arrive in 25 minutes, a regression model has predicted that number based on things like traffic, how busy the store is, and how far away you are.</p> </li> <li> <p>House Prices: Real estate websites use regression to predict how much a house is worth. They look at features like the number of bedrooms, the square footage, and the neighborhood to predict a specific price.</p> </li> </ul>"},{"location":"lessons/lesson3/#a-quick-visual-guide","title":"A Quick Visual Guide","text":"<p>Here's a little diagram to help you remember the difference:</p> <pre><code>graph TD\n    subgraph Classification [Sorting into Groups]\n        direction LR\n        A[Input Data &lt;br&gt; e.g., a picture of an animal] --&gt; B{Model};\n        B --&gt; C((Group A &lt;br&gt; 'Cat'));\n        B --&gt; D((Group B &lt;br&gt; 'Dog'));\n        B --&gt; E((Group C &lt;br&gt; 'Bird'));\n    end\n\n    subgraph Regression [Predicting a Number]\n        direction LR\n        F[Input Data &lt;br&gt; e.g., size of a house] --&gt; G{Model};\n        G --&gt; H[Predicted Value &lt;br&gt; e.g., $250,000];\n        H --&gt; I[...$240k ... $250k ... $260k...];\n    end</code></pre>"},{"location":"lessons/lesson3/#part-3-how-to-choose-your-ml-detective-kit","title":"Part 3: How to Choose - Your ML Detective Kit \ud83d\udd75\ufe0f\u200d\u2640\ufe0f","text":"<p>So, how do you know if you have a classification or a regression problem? It's easy! You just have to ask yourself one simple question.</p> <p>The One Big Question: Is the answer I want to predict a word/category, or is it a number?</p> <pre><code>graph LR\n    A{What kind of answer &lt;br&gt; do I want to predict?} --&gt; B{Is it a word or category?};\n    A --&gt; C{Is it a number?};\n    B -- Yes --&gt; D[It's Classification!];\n    C -- Yes --&gt; E[It's Regression!];</code></pre> <p>If the answer you're looking for is a label from a small group of choices (like \"pass\" or \"fail,\" \"cat\" or \"dog,\" \"spam\" or \"not spam\"), you've got a Classification problem.</p> <p>If the answer you're looking for is a number that can change (like a price, a temperature, or a score), you've got a Regression problem. It's that simple!</p>"},{"location":"lessons/lesson3/#part-4-lets-discuss","title":"Part 4: Let's Discuss!","text":"<p>Time to put on your thinking cap! For each of these problems, is it a Classification problem or a Regression problem? Use your new detective kit to decide!</p> <ol> <li> <p>Predicting whether a student will pass or fail a test.</p> </li> <li> <p>Predicting a student's exact score on that test (from 0 to 100).</p> </li> <li> <p>Predicting if a customer will buy a product (yes or no).</p> </li> <li> <p>Predicting how much money a customer will spend.</p> </li> </ol> <p>What's Next?</p> <p>You've done it again! You now know the two biggest types of problems that supervised machine learning can solve, and you even know how to tell them apart. This is a huge step in understanding how AI is used to make predictions all around us.</p> <p>Next time, we'll start to get our hands dirty with a little bit of code. We'll see how we can represent our data in a way a computer can understand and take the first steps toward building a real machine learning model.</p> <p>Stay curious!</p>"},{"location":"lessons/lesson4/","title":"Lesson 4: Getting Your Data Ready - The Secret Ingredient! \ud83c\udf73","text":"<p>Welcome back, data detective!</p> <p>In our last few lessons, we've learned what AI is, the different ways machines can learn, and the types of problems they can solve. We've talked a lot about \"data,\" but what is it, really? And how do we get it ready for a computer to understand?</p> <p>Today, we're going into the kitchen! We're going to learn how to prepare our data, which is the most important ingredient in any machine learning recipe.</p>"},{"location":"lessons/lesson4/#part-1-what-is-data-really-the-recipe-for-ml","title":"Part 1: What is Data, Really? (The Recipe for ML)","text":"<p>Think of a machine learning model as a recipe for a cake.</p> <ul> <li> <p>The ingredients you put in are your data.</p> </li> <li> <p>The final cake you bake is your prediction.</p> </li> </ul> <p>If you use bad ingredients (bad data), you're going to get a bad cake (a bad prediction), no matter how good your recipe is!</p> <p>Our data is just organized information. For our pizza-loving student problem, our data was a table of information about each student. The \"ingredients\" we used were whether they liked video games or superheroes. These ingredients are our features. The \"cake\" we were trying to bake was predicting whether they liked pizza. This is our label.</p>"},{"location":"lessons/lesson4/#part-2-from-words-to-numbers-teaching-a-computer-to-read","title":"Part 2: From Words to Numbers (Teaching a Computer to Read)","text":"<p>Here's a super important secret about computers: they can't read words, they can only understand numbers.</p> <p>If we show a computer a table with \"Yes\" and \"No,\" it has no idea what that means. We need to translate our words into a language the computer can speak. This is called encoding.</p> <p>Let's take our pizza dataset from Lesson 1.</p> <p>Before (Human-Readable):</p> Student Likes Video Games? Likes Superheroes? Likes Pizza? Alex Yes Yes Yes Ben No Yes Yes Chloe Yes No Yes David No No No <p>To translate this, we can make a simple rule:</p> <ul> <li> <p><code>Yes</code> will become <code>1</code></p> </li> <li> <p><code>No</code> will become <code>0</code></p> </li> </ul> <p>After (Computer-Readable):</p> Student Likes Video Games? Likes Superheroes? Likes Pizza? Alex 1 1 1 Ben 0 1 1 Chloe 1 0 1 David 0 0 0 <p>Now that's something a computer can work with! We've successfully encoded our features and labels into numbers.</p>"},{"location":"lessons/lesson4/#part-3-the-case-of-the-missing-clue","title":"Part 3: The Case of the Missing Clue \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>In the real world, data is often messy. What happens if you forget to ask a student a question? You end up with a missing value!</p> Student Likes Video Games? Likes Superheroes? Frank Yes ??? <p>A computer will see that <code>???</code> and have no idea what to do. It needs a complete table of numbers. We have two simple ways to solve this:</p> <ol> <li> <p>Remove the Row: The easiest option is to just remove Frank from our data. But if we have a lot of missing values, we could end up throwing away too much good information!</p> </li> <li> <p>Make a Smart Guess (Imputation): A better way is to make a smart guess. Let's look at our other students. If most of them like superheroes, it's a safe bet that Frank might, too. We can fill in the blank with the most common value.</p> </li> </ol> <p>Let's say most of our students like superheroes. We can \"impute\" the missing value:</p> <p>After (Imputed):</p> Student Likes Video Games? Likes Superheroes? (Imputed) Frank Yes Yes <p>Now our table is complete, and the computer can get back to work!</p>"},{"location":"lessons/lesson4/#part-4-the-mighty-table-features-and-labels-revisited","title":"Part 4: The Mighty Table (Features and Labels Revisited)","text":"<p>Let's look at our new, computer-friendly table. In machine learning, you'll often see this table split into two parts:</p> <ol> <li>The Features Table (The Clues): This is all the information we're using to make our prediction. It's usually called <code>X</code> in the math world.</li> </ol> Likes Video Games? Likes Superheroes? 1 1 0 1 1 0 0 0 <ol> <li>The Label Column (The Answer): This is the one thing we're trying to predict. It's usually called <code>y</code>.</li> </ol> Likes Pizza? 1 1 1 0 <p>When we build a machine learning model, we're essentially telling the computer: \"Hey, look at the patterns in the <code>X</code> table and see if you can figure out how to predict the <code>y</code> column.\"</p>"},{"location":"lessons/lesson4/#part-5-lets-discuss","title":"Part 5: Let's Discuss!","text":"<ol> <li> <p>Why is it so important to have \"good ingredients\" (good data) when building a machine learning model?</p> </li> <li> <p>Imagine you have a feature for a t-shirt size (\"Small,\" \"Medium,\" \"Large\"). Would you use One-Hot Encoding, or could you use <code>1</code>, <code>2</code>, and <code>3</code>? Why?</p> </li> <li> <p>If you wanted to predict a student's grade on a test (a regression problem), what would your <code>y</code> (label) be? What are some <code>X</code> (features) you could use to predict it?</p> </li> </ol> <p>What's Next?</p> <p>This is a huge step! You now understand the secret language of data that computers use. You know how to take real-world information, handle messy situations, and turn it all into numbers that a machine learning model can learn from.</p> <p>You are officially ready to build your first real machine learning model. In our next lesson, we'll use a simple but powerful tool to take our encoded pizza data and create a model that can make predictions, all with just a few lines of code!</p> <p>Get excited!</p>"},{"location":"lessons/lesson5/","title":"Lesson 5: Your First AI Model - The Super-Smart Flowchart! \ud83c\udf33","text":"<p>Welcome back, future AI architect!</p> <p>This is the moment we've been building up to. We've learned what AI is, how machines learn, and how to prepare our data. Today, we're going to combine all that knowledge and build our very first real machine learning model using code!</p> <p>Don't worry, we're going to start with a model that's super intuitive and easy to understand: the Decision Tree.</p>"},{"location":"lessons/lesson5/#part-1-what-is-a-decision-tree-a-20-questions-game","title":"Part 1: What is a Decision Tree? (A \"20 Questions\" Game)","text":"<p>A Decision Tree is basically a flowchart that the computer learns all by itself. It's like playing a game of \"20 Questions.\" You ask a series of yes/no questions to narrow down the possibilities and arrive at a final answer.</p> <p>Imagine you're trying to guess an animal. You might ask:</p> <ul> <li> <p>\"Does it live in the water?\" (No)</p> </li> <li> <p>\"Does it have four legs?\" (Yes)</p> </li> <li> <p>\"Does it bark?\" (Yes)</p> </li> <li> <p>\"It must be a dog!\"</p> </li> </ul> <p>A Decision Tree does the exact same thing with data! It learns the best questions to ask to separate the data into different groups.</p>"},{"location":"lessons/lesson5/#part-2-building-the-tree-from-data-to-flowchart","title":"Part 2: Building the Tree (From Data to Flowchart)","text":"<p>Let's go back to our trusty pizza-lover dataset. Remember, we encoded it into numbers:</p> Likes Video Games? Likes Superheroes? Likes Pizza? 1 1 1 0 1 1 1 0 1 0 0 0 <p>A Decision Tree model would look at this data and figure out the best question to ask first. It might learn that asking \"Likes Video Games?\" is a good starting point. It then splits the data into two groups based on the answer. It continues asking questions until it has sorted all the students.</p> <p>The result is a flowchart that the computer builds automatically. It might look something like this:</p> <pre><code>graph TD\n    A{Likes Video Games?} --&gt;|\"Yes (1)\"| B{Likes Superheroes?};\n    A --&gt;|\"No (0)\"| C{Likes Superheroes?};\n    B --&gt;|\"Yes (1)\"| D[\"Predict: Likes Pizza! (1)\"];\n    B --&gt;|\"No (0)\"| E[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"Yes (1)\"| F[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"No (0)\"| G[\"Predict: Does NOT Like Pizza! (0)\"];</code></pre> <p>This is the \"brain\" of our model! It's a set of rules the computer has learned from the data.</p>"},{"location":"lessons/lesson5/#part-3-making-a-prediction-following-the-branches","title":"Part 3: Making a Prediction (Following the Branches)","text":"<p>Now, let's use our tree to predict if a new student, Frank, will like pizza.</p> <ul> <li>Frank's Data: He likes video games (<code>1</code>), but not superheroes (<code>0</code>).</li> </ul> <p>We start at the top of the tree:</p> <ol> <li> <p>Likes Video Games? Frank's answer is Yes. We follow the \"Yes\" branch.</p> </li> <li> <p>Likes Superheroes? Frank's answer is No. We follow the \"No\" branch.</p> </li> <li> <p>Final Answer: We land on a prediction: Likes Pizza!</p> </li> </ol> <p>It's that simple! We just followed the path down the tree to get our answer.</p>"},{"location":"lessons/lesson5/#part-4-the-code-part-python-scikit-learn","title":"Part 4: The \"Code\" Part (Python &amp; Scikit-learn)","text":"<p>Okay, it's time. How do we actually tell a computer to do this? We use a programming language, and the most popular one for AI is Python.</p> <p>But we don't have to build everything from scratch! We can use a special \"toolbox\" for machine learning called scikit-learn. It's a free library that has all the common ML models, including Decision Trees, ready to go.</p> <p>Here\u2019s what the code would look like. Don't worry about understanding every single word. Just focus on the three main steps and the comments that explain them.</p> <pre><code># Step 1: Prepare the data (just like we did in Lesson 4!)\n# X is our features (the clues), y is our label (the answer)\nX = [[1, 1], [0, 1], [1, 0], [0, 0]]  # [Likes Games, Likes Heroes]\ny = [1, 1, 1, 0]                     # Likes Pizza (1=Yes, 0=No)\n\n# Step 2: Create and train the model\n# We'll import the Decision Tree model from our scikit-learn toolbox\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a new, empty Decision Tree model\nmy_model = DecisionTreeClassifier()\n\n# This is the magic \"learning\" step!\n# We tell the model to \"fit\" itself to our data.\nmy_model.fit(X, y)\n\n# Step 3: Make a prediction for a new student\n# Let's test it on Frank, who likes games (1) but not heroes (0)\nfrank = [[1, 0]]\nprediction = my_model.predict(frank)\n\n# See the result!\nif prediction == 1:\n    print(\"The model predicts Frank will like pizza!\")\nelse:\n    print(\"The model predicts Frank will NOT like pizza.\")\n\n# The output will be: The model predicts Frank will like pizza!\n</code></pre>"},{"location":"lessons/lesson5/#a-closer-look-what-do-fit-and-predict-actually-do","title":"A Closer Look: What Do <code>.fit()</code> and <code>.predict()</code> Actually Do?","text":"<p>Those two lines of code, <code>my_model.fit(X, y)</code> and <code>my_model.predict(frank)</code>, are the heart of the whole process. Let's use an analogy to understand them better.</p> <ul> <li> <p><code>.fit(X, y)</code> is for FORGING THE KEY \ud83d\udd11     This is the \"study\" or \"training\" phase. Think of it like a locksmith studying a lock (the <code>y</code> labels) and the original key (<code>X</code> features). The locksmith looks at all the bumps and grooves to understand the pattern. <code>fit</code> is the moment the computer does the same thing with our data. It looks at our features and labels and builds the Decision Tree flowchart. Before <code>.fit()</code>, <code>my_model</code> is just an empty shell. After <code>.fit()</code>, it's a fully trained model that has forged a \"key\" based on the patterns in the data.</p> </li> <li> <p><code>.predict(new_data)</code> is for UNLOCKING THE DOOR \ud83d\udeaa     This is the \"doing\" or \"testing\" phase. Now you have a new door (<code>frank</code>'s data), but you don't know what's behind it. You take the key you just forged (<code>my_model</code>) and use it on the new lock. <code>.predict()</code> is the action of turning the key. The result\u2014whether the door opens or not\u2014is your prediction!</p> </li> </ul>"},{"location":"lessons/lesson5/#part-5-lets-discuss","title":"Part 5: Let's Discuss!","text":"<ol> <li>Think about how you decide what to wear in the morning. What \"decision tree\" do you use? (e.g., Is it cold? -&gt; Yes -&gt; Wear a jacket).</li> <li>Our tree is very simple. What could go wrong if we used it to predict for all new students? (Hint: Think about Grace from Lesson 1).</li> <li>Does the code seem more or less complicated than you expected? What part is the most surprising?</li> </ol> <p>What's Next?</p> <p>CONGRATULATIONS! You have officially learned how to build and train your very first AI model. You've taken data, chosen a model, trained it, and used it to make a prediction. This is the core of what machine learning is all about.</p> <p>From here, you can explore other types of models, learn how to handle much bigger datasets, and even figure out how to measure if your model is any good. The journey is just beginning!</p> <p>You are no longer just a student of AI\u2014you are a builder. Stay curious!</p>"},{"location":"lessons/lesson6/","title":"Lesson 6: Under the Hood - Building a Decision Tree by Hand! \ud83d\udee0\ufe0f","text":"<p>Welcome back, AI builder!</p> <p>In our last lesson, we used the power of <code>scikit-learn</code> to build a Decision Tree model in just a few lines of code. It felt a bit like magic, right? You call <code>.fit()</code> and suddenly the computer has \"learned.\"</p> <p>Today, we're going to become the magicians. We're going to pull back the curtain and see what's happening inside that <code>.fit()</code> method. We'll build our very own Decision Tree from scratch using nothing but basic Python.</p>"},{"location":"lessons/lesson6/#part-1-from-flowchart-to-code","title":"Part 1: From Flowchart to Code","text":"<p>Let's look at our Decision Tree flowchart from the last lesson one more time.</p> <pre><code>graph TD\n    A{Likes Video Games?} --&gt;|\"Yes (1)\"| B{Likes Superheroes?};\n    A --&gt;|\"No (0)\"| C{Likes Superheroes?};\n    B --&gt;|\"Yes (1)\"| D[\"Predict: Likes Pizza! (1)\"];\n    B --&gt;|\"No (0)\"| E[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"Yes (1)\"| F[\"Predict: Likes Pizza! (1)\"];\n    C --&gt;|\"No (0)\"| G[\"Predict: Does NOT Like Pizza! (0)\"];</code></pre> <p>This flowchart is just a set of rules. We can read it like a story: \"First, ask if the student likes video games. If the answer is yes, then ask if they like superheroes...\" and so on.</p> <p>We can translate this exact story into Python using <code>if/else</code> statements.</p>"},{"location":"lessons/lesson6/#part-2-the-rules-in-python","title":"Part 2: The \"Rules\" in Python","text":"<p>Let's create our own prediction function called <code>predict_pizza_by_hand</code>. This function will take a student's data as a list (e.g., <code>[1, 0]</code>) and follow our rules.</p> <p>The first element in the list (<code>student[0]</code>) will be \"Likes Video Games?\", and the second element (<code>student[1]</code>) will be \"Likes Superheroes?\".</p> <pre><code>def predict_pizza_by_hand(student):\n    # The first question in our flowchart\n    if student[0] == 1:  # Do they like video games?\n        # If YES, we go down the left side of the tree.\n        # Now we ask the second question.\n        if student[1] == 1:  # Do they also like superheroes?\n            return \"Likes Pizza!\"\n        else:  # They don't like superheroes.\n            return \"Likes Pizza!\"\n    else:  # They do NOT like video games.\n        # If NO, we go down the right side of the tree.\n        # Now we ask the second question.\n        if student[1] == 1:  # Do they like superheroes?\n            return \"Likes Pizza!\"\n        else:  # They don't like superheroes either.\n            return \"Does NOT Like Pizza!\"\n</code></pre> <p>Look at that! The <code>if/else</code> statements are a perfect mirror of the branches in our flowchart. We've just manually coded the \"brain\" of our Decision Tree.</p>"},{"location":"lessons/lesson6/#part-3-putting-it-all-together","title":"Part 3: Putting It All Together","text":"<p>Now let's use our hand-built model to make a prediction, just like we did in the last lesson.</p> <pre><code># This is our hand-built model!\ndef predict_pizza_by_hand(student):\n    if student[0] == 1:\n        if student[1] == 1:\n            return \"Likes Pizza!\"\n        else:\n            return \"Likes Pizza!\"\n    else:\n        if student[1] == 1:\n            return \"Likes Pizza!\"\n        else:\n            return \"Does NOT Like Pizza!\"\n\n# Let's test it on Frank, who likes games (1) but not heroes (0)\nfrank = [1, 0]\nprediction = predict_pizza_by_hand(frank)\n\n# See the result!\nprint(\"Our hand-built model predicts Frank:\", prediction)\n\n# Let's test it on David, who doesn't like games (0) or heroes (0)\ndavid = [0, 0]\nprediction = predict_pizza_by_hand(david)\nprint(\"Our hand-built model predicts David:\", prediction)\n\n# The output will be:\n# Our hand-built model predicts Frank: Likes Pizza!\n# Our hand-built model predicts David: Does NOT Like Pizza!\n</code></pre> <p>It works perfectly! We have successfully created a machine learning model from scratch that can make predictions based on our specific rules.</p>"},{"location":"lessons/lesson6/#part-4-the-aha-moment-why-we-use-toolboxes","title":"Part 4: The \"Aha!\" Moment - Why We Use Toolboxes","text":"<p>This is great, but you might be thinking: \"If we can do this by hand, why do we need <code>scikit-learn</code>?\"</p> <p>That's the most important question!</p> <p>Our hand-built model is not learning. We looked at the data and we decided what the rules should be. We hard-coded the <code>if/else</code> statements ourselves. If we got new data, our model wouldn't change.</p> <p>The magic of <code>scikit-learn</code>'s <code>.fit()</code> method is that it does all of this for us! It looks at the data and automatically discovers the best <code>if/else</code> rules to use. It figures out the best questions to ask and in what order to make the most accurate predictions.</p> <p>If our data had 100 features, we would go crazy trying to write all the <code>if/else</code> statements by hand. But <code>.fit()</code> can figure it out in a fraction of a second.</p> <p>So, by building a model by hand, you've learned the secret of what a Decision Tree really is: it's just a smart set of <code>if/else</code> rules. And now you can truly appreciate why toolboxes like <code>scikit-learn</code> are so powerful\u2014they are masters at finding those rules for us.</p>"},{"location":"lessons/lesson6/#part-5-building-a-real-learning-algorithm","title":"Part 5: Building a Real Learning Algorithm","text":"<p>The Challenge: Welcome to the deep end of the pool! The rest of this lesson is a challenge. It's a peek into how a real learning algorithm works. It uses some more advanced ideas, but if you take it slow, you'll be able to see the real magic behind machine learning. Don't worry if you don't get it all on the first try\u2014the goal is to see how it's done!</p>"},{"location":"lessons/lesson6/#tool-1-recursion-the-endless-mirror","title":"Tool 1: Recursion (The Endless Mirror)","text":"<p>Imagine you're standing between two mirrors. You see a reflection of a reflection of a reflection... forever! That's recursion. In programming, it's a function that calls itself to solve a smaller piece of the same problem. We'll use this to \"grow\" our tree, with each branch being a smaller version of the tree.</p>"},{"location":"lessons/lesson6/#tool-2-data-structures-the-trees-blueprint","title":"Tool 2: Data Structures (The Tree's Blueprint)","text":"<p>How do we store a tree in code? We can use a dictionary or a class! Each \"node\" (or question) in our tree can hold the question and its \"left\" and \"right\" branches, which are more nodes.</p>"},{"location":"lessons/lesson6/#tool-3-gini-impurity-the-purity-score","title":"Tool 3: Gini Impurity (The \"Purity\" Score)","text":"<p>This is the secret sauce. How does the algorithm know which question is best? It uses a \"purity score\" to check how well a question splits the data.</p> <ul> <li> <p>A perfectly pure group has a score of 0 (e.g., a group of students who all like pizza).</p> </li> <li> <p>A totally mixed group has a score of 0.5 (e.g., a group with 50% pizza lovers and 50% pizza haters).</p> </li> </ul> <p>The algorithm calculates the purity score for every possible question and picks the one that creates the purest groups.</p>"},{"location":"lessons/lesson6/#the-code-step-by-step-a-visual-walkthrough","title":"The Code, Step-by-Step: A Visual Walkthrough","text":"<p>Let's trace the computer's \"thinking\" as it starts to build the tree.</p> <p>Input: Our full dataset of 4 students. <pre><code>[[1, 1, 1],\n [0, 1, 1],\n [1, 0, 1],\n [0, 0, 0]]\n</code></pre> The computer's first goal is to find the single best question to ask to split this group.</p> <p>Step 1: Calculate the Starting Purity First, the computer looks at the whole group. There are 3 pizza lovers (1) and 1 hater (0). This is a pretty mixed group.</p> <ul> <li> <p>Computation: The Gini Impurity is <code>1 - ((3/4)^2 + (1/4)^2) = 1 - (0.5625 + 0.0625) = 0.375</code>.</p> </li> <li> <p>Output: The starting \"impurity\" is 0.375. The goal is to ask a question that lowers this number as much as possible.</p> </li> </ul> <pre><code>graph TD\n    subgraph Starting Group\n        A[\"[1,1,1], [0,1,1], [1,0,1], [0,0,0]\"]\n    end\n    B[\"Gini = 0.375\"]\n    A --&gt; B</code></pre> <p>Step 2: Test the First Question (\"Likes Video Games?\") The computer pretends to split the group based on this question.</p> <pre><code>graph TD\n    subgraph \"Split on 'Likes Video Games?'\"\n        A[\"Parent Gini = 0.375\"] --&gt; B{Likes Games?};\n        B --&gt;|Yes| C[\"Left Group: [1,1,1], [1,0,1]&lt;br&gt;2 Likes Pizza, 0 Hates&lt;br&gt;&lt;b&gt;Gini = 0.0 (Perfectly Pure!)&lt;/b&gt;\"];\n        B --&gt;|No| D[\"Right Group: [0,1,1], [0,0,0]&lt;br&gt;1 Likes Pizza, 1 Hates&lt;br&gt;&lt;b&gt;Gini = 0.5 (Totally Mixed!)&lt;/b&gt;\"];\n    end</code></pre> <ul> <li> <p>Computation: The computer calculates a weighted average of the new impurity: <code>(2/4)*0.0 + (2/4)*0.5 = 0.25</code>. The \"Information Gain\" (how much the impurity went down) is <code>0.375 - 0.25 = 0.125</code>.</p> </li> <li> <p>Output: The score for this question is 0.125.</p> </li> </ul> <p>Step 3: Test the Second Question (\"Likes Superheroes?\") Now the computer \"forgets\" the last split and tries again with the next question.</p> <pre><code>graph TD\n    subgraph \"Split on 'Likes Superheroes?'\"\n        A[\"Parent Gini = 0.375\"] --&gt; B{Likes Heroes?};\n        B --&gt;|Yes| C[\"Left Group: [1,1,1], [0,1,1]&lt;br&gt;2 Likes Pizza, 0 Hates&lt;br&gt;&lt;b&gt;Gini = 0.0 (Perfectly Pure!)&lt;/b&gt;\"];\n        B --&gt;|No| D[\"Right Group: [1,0,1], [0,0,0]&lt;br&gt;1 Likes Pizza, 1 Hates&lt;br&gt;&lt;b&gt;Gini = 0.5 (Totally Mixed!)&lt;/b&gt;\"];\n    end</code></pre> <ul> <li> <p>Computation: The weighted average is the same: <code>(2/4)*0.0 + (2/4)*0.5 = 0.25</code>. The \"Information Gain\" is also <code>0.375 - 0.25 = 0.125</code>.</p> </li> <li> <p>Output: The score for this question is 0.125.</p> </li> </ul> <p>Step 4: The Decision The computer compares the scores. In this case, they're tied! In a tie, the computer just picks the first one it tried.</p> <ul> <li>The Winner: The best first question is \"Likes Video Games?\".</li> </ul> <p>Step 5: The Recursion The computer now has the root of its tree! It then repeats this entire process on the smaller groups.</p> <ol> <li> <p>It runs the <code>get_best_split</code> logic on the \"Yes\" group.</p> </li> <li> <p>It runs the <code>get_best_split</code> logic on the \"No\" group.</p> </li> </ol> <p>This is the \"endless mirror\" of recursion in action! It continues until the groups are pure.</p> <p>This whole process of testing splits and finding the one that lowers the impurity score the most is the \"magic\" inside the <code>.fit()</code> method!</p>"},{"location":"lessons/lesson6/#the-full-code-a-real-decision-tree","title":"The Full Code: A Real Decision Tree","text":"<p>This code puts all those ideas together. It's complex, but now you can see the \"thinking\" that happens inside it.</p> <pre><code>import numpy as np\n\n# A class to represent a single node in our decision tree\nclass Node:\n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.info_gain = info_gain\n        self.value = value\n\n# The main Decision Tree class\nclass MyDecisionTree:\n    def __init__(self, min_samples_split=2, max_depth=2):\n        self.root = None\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n\n    def build_tree(self, dataset, curr_depth=0):\n        X, Y = dataset[:,:-1], dataset[:,-1]\n        num_samples, num_features = np.shape(X)\n\n        if num_samples &gt;= self.min_samples_split and curr_depth &lt;= self.max_depth:\n            best_split = self.get_best_split(dataset, num_samples, num_features)\n            if best_split[\"info_gain\"] &gt; 0:\n                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n                            left_subtree, right_subtree, best_split[\"info_gain\"])\n\n        leaf_value = self.calculate_leaf_value(Y)\n        return Node(value=leaf_value)\n\n    def get_best_split(self, dataset, num_samples, num_features):\n        best_split = {}\n        max_info_gain = -float(\"inf\")\n\n        for feature_index in range(num_features):\n            feature_values = dataset[:, feature_index]\n            possible_thresholds = np.unique(feature_values)\n            for threshold in possible_thresholds:\n                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n                if len(dataset_left) &gt; 0 and len(dataset_right) &gt; 0:\n                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n                    curr_info_gain = self.information_gain(y, left_y, right_y)\n                    if curr_info_gain &gt; max_info_gain:\n                        best_split[\"feature_index\"] = feature_index\n                        best_split[\"threshold\"] = threshold\n                        best_split[\"dataset_left\"] = dataset_left\n                        best_split[\"dataset_right\"] = dataset_right\n                        best_split[\"info_gain\"] = curr_info_gain\n                        max_info_gain = curr_info_gain\n        return best_split\n\n    def split(self, dataset, feature_index, threshold):\n        dataset_left = np.array([row for row in dataset if row[feature_index] &lt;= threshold])\n        dataset_right = np.array([row for row in dataset if row[feature_index] &gt; threshold])\n        return dataset_left, dataset_right\n\n    def information_gain(self, parent, l_child, r_child):\n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        gain = self.gini_impurity(parent) - (weight_l * self.gini_impurity(l_child) + weight_r * self.gini_impurity(r_child))\n        return gain\n\n    def gini_impurity(self, y):\n        class_labels = np.unique(y)\n        gini = 0\n        for cls in class_labels:\n            p_cls = len(y[y == cls]) / len(y)\n            gini += p_cls**2\n        return 1 - gini\n\n    def calculate_leaf_value(self, Y):\n        Y = list(Y)\n        return max(Y, key=Y.count)\n\n    def fit(self, X, Y):\n        dataset = np.concatenate((X, Y), axis=1)\n        self.root = self.build_tree(dataset)\n\n    def predict(self, X):\n        predictions = [self.make_prediction(x, self.root) for x in X]\n        return predictions\n\n    def make_prediction(self, x, tree):\n        if tree.value != None: return tree.value\n        feature_val = x[tree.feature_index]\n        if feature_val &lt;= tree.threshold:\n            return self.make_prediction(x, tree.left)\n        else:\n            return self.make_prediction(x, tree.right)\n\n# Let's use it!\nX = np.array([[1, 1], [0, 1], [1, 0], [0, 0]])\nY = np.array([[1], [1], [1], [0]])\n\nmy_tree = MyDecisionTree(min_samples_split=2, max_depth=2)\nmy_tree.fit(X, Y)\n\nfrank = [[1, 0]]\nprediction = my_tree.predict(frank)\nprint(\"Our REAL hand-built model predicts Frank's result is:\", prediction)\n</code></pre>"},{"location":"lessons/lesson6/#part-6-lets-discuss","title":"Part 6: Let's Discuss!","text":"<ol> <li>What do you think is the biggest advantage of using a library like <code>scikit-learn</code> now that you've seen the complexity behind it?</li> <li>Our Gini Impurity calculation was simple for our small dataset. How would this process change for a dataset with millions of rows?</li> <li>Does seeing the \"from scratch\" version make the <code>scikit-learn</code> code from Lesson 5 make more sense?</li> </ol> <p>What's Next?</p> <p>This is the end of our introductory journey, but it's just the beginning of your adventure in AI.</p> <p>You have learned:</p> <ul> <li> <p>What AI, ML, and DL are.</p> </li> <li> <p>The different ways machines can learn.</p> </li> <li> <p>How to prepare data for a computer.</p> </li> <li> <p>How to build a model with a powerful toolbox.</p> </li> <li> <p>And now, you've even built a real learning algorithm from scratch!</p> </li> </ul> <p>You have a stronger foundation in machine learning than most people on the planet. The next step is to keep building. Keep asking questions. And most importantly, stay curious!</p>"}]}